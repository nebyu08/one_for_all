{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d42334e-bac7-44d7-979a-aa11af5f0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef8ab7e-2531-412e-b9ce-ee656aded022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from activation import Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71267c3-540a-4019-ab70-8fc8f5940997",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58649efb-6e21-41c1-bf50-be1bfb0e4af0",
   "metadata": {},
   "source": [
    "### forwardpass of single neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58109a5-e1ca-4796-9865-8b3bed4422a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1,2,3]\n",
    "w=[-3,-1,2]\n",
    "b=1\n",
    "\n",
    "xw0=x[0]*w[0]\n",
    "xw1=x[1]*w[1]\n",
    "xw2=x[2]*w[2]\n",
    "\n",
    "z=xw0+xw1+xw2+b  #after adding the bias \n",
    "\n",
    "#lets pass through the activation function\n",
    "z=max(0,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da74fe0e-6ceb-4429-b1ec-bd443fadf5bc",
   "metadata": {},
   "source": [
    "### lets check out the backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2540c3bf-44dc-4dd5-abeb-4898f1234b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1 1 1 1\n",
      "-3 1 -1 -2 2 3\n"
     ]
    }
   ],
   "source": [
    "x=[1,-2,3]\n",
    "w=[-3,-1,2]\n",
    "b=1\n",
    "\n",
    "xw0=x[0]*w[0]\n",
    "xw1=x[1]*w[1]\n",
    "xw2=x[2]*w[2]\n",
    "\n",
    "z=xw0+xw1+xw2+b\n",
    "\n",
    "#lets pass it through the relu activation function\n",
    "y=max(0,z)\n",
    "\n",
    "#let check the backpropagation\n",
    "\n",
    "#the value of the Relu \n",
    "dvalue=1  \n",
    "\n",
    "#the derivative from the next layer and Relu function\n",
    "drelu_dz=dvalue*(1 if z>0 else 0)\n",
    "\n",
    "#lets print our findings\n",
    "print(drelu_dz)\n",
    "\n",
    "#lets backpropagate through the summation\n",
    "dz_dxw0=1\n",
    "drelu_xw0=drelu_dz*dz_dxw0\n",
    "\n",
    "dz_dxw1=1\n",
    "drelu_xw1=drelu_dz*dz_dxw1\n",
    "\n",
    "dz_dxw2=1\n",
    "drelu_xw2=drelu_dz*dz_dxw2\n",
    "\n",
    "dz_db=1\n",
    "drelu_db=drelu_dz*dz_db\n",
    "#lets print ourfindings\n",
    "print(drelu_xw0,drelu_xw1,drelu_xw2,drelu_db)\n",
    "\n",
    "#lets backpropagate through the inputs\n",
    "dxw0_dx0=w[0]\n",
    "drelu_dx0=drelu_dz*dz_dxw0*dxw0_dx0\n",
    "\n",
    "dxw1_dx1=w[1]\n",
    "drelu_dx1=drelu_dz*dz_dxw1*dxw1_dx1\n",
    "\n",
    "dxw2_dx2=w[2]\n",
    "drelu_dx2=drelu_dz*dz_dxw2*dxw2_dx2\n",
    "\n",
    "#lets backpropgate through the weights\n",
    "dxw0_dw0=x[0]\n",
    "drelu_dw0=drelu_dz*dz_dxw0*dxw0_dw0\n",
    "\n",
    "dxw1_dw1=x[1]\n",
    "drelu_dw1=drelu_dz*dz_dxw1*dxw1_dw1\n",
    "\n",
    "dxw2_dw2=x[2]\n",
    "drelu_dw2=drelu_dz*dz_dxw2*dxw2_dw2\n",
    "\n",
    "\n",
    "#lets print our backpropgated values\n",
    "print(drelu_dx0,drelu_dw0,drelu_dx1,drelu_dw1,drelu_dx2,drelu_dw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171d0ec5-80a4-483f-be7f-acb7884789f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# the output\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cccc1-dc10-4771-bc12-4653534eaa15",
   "metadata": {},
   "source": [
    "#### yay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e4ccdd-e42e-4000-9457-f4b07e181b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our gradients are\n",
    "dw=[drelu_dw0,drelu_dw1,drelu_dw2]\n",
    "dx=[drelu_dx0,drelu_dx1,drelu_dx2]\n",
    "db=drelu_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "595a6878-ebdd-4a9c-99ec-4ac931a6b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3, -1, 2]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf5db703-4a1d-4a6a-a313-f985e18a436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(w))\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56809acf-4549-4069-91af-b6290f4f36e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.985\n"
     ]
    }
   ],
   "source": [
    "# lets update the parameter of the values\n",
    "w[0]+=-0.001*dw[0]\n",
    "w[1]+=-0.001*dw[1]\n",
    "w[2]+=-0.001*dw[2]\n",
    "b+=-0.001*db\n",
    "\n",
    "#lets train our model again and see\n",
    "xw0=x[0]*w[0]\n",
    "xw1=x[1]*w[1]\n",
    "xw2=x[2]*w[2]\n",
    "z=xw0+xw1+xw2+b\n",
    "y=max(0,z)\n",
    "\n",
    "#output\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12ef52-047a-40ab-b50f-9433f1ff0871",
   "metadata": {},
   "source": [
    "# move on to layer that has multiple neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7efc72b9-0321-448c-a7f4-97e54720f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44  0.44  0.44]\n",
      " [-0.38 -0.38 -0.38]\n",
      " [-0.07 -0.07 -0.07]\n",
      " [ 1.37  1.37  1.37]]\n"
     ]
    }
   ],
   "source": [
    "#backpropagated from next layers\n",
    "dvalues=np.array([[1,1,1]])\n",
    "\n",
    "weights=np.array([\n",
    "    [0.2,0.8,-0.5,1],\n",
    "    [0.5,-0.91,0.26,-0.5],\n",
    "    [-0.26,-0.27,0.17,0.87]\n",
    "]).T    # Notice:we are transposing the values here please pay attention here\n",
    "\n",
    "\n",
    "#lets implement the backpropagation with respect to the inputs\n",
    "dx0=sum(weights[0])*dvalues[0]\n",
    "dx1=np.sum(weights[1])*dvalues[0]\n",
    "dx2=np.sum(weights[2])*dvalues[0]\n",
    "dx3=np.sum(weights[3])*dvalues[0]\n",
    "\n",
    "dinputs=np.array([dx0,dx1,dx2,dx3]) #the derivatives of all the inputs\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935c62aa-c7e3-4704-b40a-a727a42a6fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43999999999999995\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(weights[0])*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e38f2499-bbba-4a5c-926b-726d79eb38fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "temp=np.array([[1,2,3]])\n",
    "print(temp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd421e48-b60d-4322-9f18-dd6292d92359",
   "metadata": {},
   "source": [
    "## more effificent way of making the derivartive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af67c27b-ed86-40b7-a5fd-9ff62d9f32cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "#a better way of running this code \n",
    "dx0=sum(weights[0]*dvalues[0])\n",
    "dx1=sum(weights[1]*dvalues[0])\n",
    "dx2=sum(weights[2]*dvalues[0])\n",
    "dx3=sum(weights[3]*dvalues[0])\n",
    "\n",
    "dinputs=np.array([dx0,dx1,dx2,dx3])\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cf490a-b0b4-47ef-a1c9-b13a830035c1",
   "metadata": {},
   "source": [
    "## using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be9f34b4-e875-4783-bbc4-2c771524c5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "#lets ease the calculation using numpy\n",
    "dinputs=np.dot(dvalues[0],weights.T)   # 1x3 weights 3x4\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf8e1a1-5f73-452e-8bc2-dfa3af865d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (1, 3))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape,dvalues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcdf2701-1adb-40d7-8a4d-d907d7f1c22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "temp=np.dot(weights,dvalues[0].T)\n",
    "#print(temp,temp.shape)\n",
    "#print()\n",
    "print(temp.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b4176-f10d-4847-bcc4-8774f86537de",
   "metadata": {},
   "source": [
    "# handling batch of data during backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953eaae-3c05-40db-9b61-5555e4fa6b82",
   "metadata": {},
   "source": [
    "## derivating with respect to inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be404f46-5fce-43b6-b704-3db81336613b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "#derivatives from the next layer\n",
    "dvalues=np.array([\n",
    "    [1,1,1],\n",
    "    [2,2,2],\n",
    "    [3,3,3]\n",
    "])\n",
    "weights=np.array([\n",
    "    [0.2,0.8,-0.5,1],\n",
    "    [0.5,-0.91,0.26,-0.5],\n",
    "    [-0.26,-0.27,0.17,0.87]\n",
    "]).T\n",
    "\n",
    "dinputs=np.dot(dvalues,weights.T)  #dvalues shape is=3x3 and weights is 4x3\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f91e5-ace0-4e9c-bde4-b48d0a63a107",
   "metadata": {},
   "source": [
    "## lets derivate with respect to the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "505da548-f2fe-4901-a28a-ed1f855a193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "#the values that is backpropagated through the network\n",
    "dvalues=np.array([\n",
    "    [1,1,1],\n",
    "    [2,2,2],\n",
    "    [3,3,3]\n",
    "])\n",
    "\n",
    "#inputs\n",
    "inputs=np.array([\n",
    "    [1,2,3,2.5],\n",
    "    [2,5,-1,2],\n",
    "    [-1.5,2.7,3.3,-0.8]\n",
    "])\n",
    "\n",
    "dweights=np.dot(inputs.T,dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73434a1-1af5-4e3a-b8ae-77ed13975694",
   "metadata": {},
   "source": [
    "# derivate with respect to the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fffcef5d-203b-444a-bec0-32f4f9dee5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "#lets go\n",
    "biases=np.array([[2,3,0.5]])\n",
    "\n",
    "dvalues=np.array([\n",
    "    [1,1,1],\n",
    "    [2,2,2],\n",
    "    [3,3,3]\n",
    "])\n",
    "\n",
    "dbias=np.sum(dvalues,axis=0,keepdims=True)  #summing column wise here and makes it row vector\n",
    "print(dbias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3caf313-0a82-43c0-949f-b08dc853ecb4",
   "metadata": {},
   "source": [
    "# derivate with respect to the Relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c070ef0-07ea-4400-807b-8092b5784fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z=np.array([\n",
    "#     [1,2,-3,-4],\n",
    "#     [2,-7,-1,3],\n",
    "#     [-1,2,5,-1]\n",
    "# ])\n",
    "# print(z)\n",
    "\n",
    "# dvalues=np.array([\n",
    "#     [1,2,3,4],\n",
    "#     [5,6,7,8],\n",
    "#     [9,10,11,12]\n",
    "# ])\n",
    "\n",
    "# print(dvalues)\n",
    "\n",
    "# #les setup the shape of the derivative with relu\n",
    "# drelu=np.zeros_like(z)\n",
    "#print(drelu.shape)\n",
    "\n",
    "# def sth(z):\n",
    "#      return list(map(lambda x:1 if x > 0 else 0 , z))\n",
    "    \n",
    "# drelu_res = map(sth, z)\n",
    "# drelu[z>0]=1\n",
    "# for i in z:\n",
    "#     print(i)\n",
    "#drelu = [[1 for j in i if j > 0] for i in z]\n",
    "# for i, k in enumerate(z):\n",
    "#     for j, x in enumerate(k):\n",
    "#         if x > 0: drelu[i][j] = 1\n",
    "            \n",
    "# print(type(drelu))\n",
    "# print(list(drelu_res))\n",
    "\n",
    "#print(drelu)\n",
    "# drelu*=dvalues\n",
    "#print(drelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f004020b-af01-4092-b1ec-7b07ca3ccbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=np.array([\n",
    "    [1,2,-3,-4],\n",
    "    [2,-7,-1,3],\n",
    "    [-1,2,5,-1]\n",
    "])\n",
    "\n",
    "dvalues=np.array([\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12]\n",
    "])\n",
    "\n",
    "#set the shape\n",
    "drelu=np.zeros_like(z)\n",
    "\n",
    "#make mappping fromt the input to the gradient\n",
    "drelu[z>0]=1\n",
    "\n",
    "drelu*=dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86eeb143-f715-40d4-ab57-56ebf8ecd89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "print(drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7e35a-6249-4996-ab98-617e1cda58ad",
   "metadata": {},
   "source": [
    "## another way of doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "523ef537-d365-4d82-922b-e2045bffc224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "z=np.array([\n",
    "    [1,2,-3,-4],\n",
    "    [2,-7,-1,3],  \n",
    "    [-1,2,5,-1]\n",
    "])\n",
    "\n",
    "dvalues=np.array([\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8],\n",
    "    [9,10,11,12]\n",
    "])\n",
    "\n",
    "drelu=dvalues.copy()  #if we dont copy it..it becomes pointer like data relationship\n",
    "drelu[z<0]=0\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01dd2ce-0115-4a78-b268-8f7ae993cef0",
   "metadata": {},
   "source": [
    "# lets see for batch based gradient in layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "995c004d-be78-44cf-86a6-94d9c42af54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start\n",
    "dvalues=np.array([\n",
    "    [1,1,1],\n",
    "    [2,2,2],\n",
    "    [3,3,3]\n",
    "])\n",
    "\n",
    "#bacth of inputs like 3 of them hehe\n",
    "inputs=np.array([\n",
    "    [1,2,3,2.5],\n",
    "    [2,5,-1,2],\n",
    "    [-1.5,2.7,3.3,-0.8]\n",
    "])\n",
    "weights=np.array([\n",
    "    [0.2,0.8,-0.5,1],\n",
    "    [0.5,-0.91,0.26,-0.5],\n",
    "    [0.26,-0.27,0.17,0.87]\n",
    "]).T     #the origina shape before transpose is number of neurons by the input features\n",
    "\n",
    "bias=np.array([[2,3,0.5]])\n",
    "\n",
    "layer_output=np.dot(inputs,weights)+bias\n",
    "\n",
    "relu_output=np.maximum(0,layer_output)\n",
    "#print(relu_output)\n",
    "drelu=dvalues.copy()\n",
    "drelu[layer_output<0]=0\n",
    "#print(drelu)\n",
    "\n",
    "#lets get deep derivation using chain rul\n",
    "dinputs=np.dot(drelu,weights.T)\n",
    "dweights=np.dot(inputs.T,drelu)\n",
    "dbiases=np.sum(drelu,axis=0)  #summing along the column\n",
    "\n",
    "#lets update the parameter\n",
    "weights+=-0.01*dweights\n",
    "bias+=-0.01*dbias\n",
    "\n",
    "# print(weights)\n",
    "# print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a131f-0d4d-461e-a4af-400aa0df0799",
   "metadata": {},
   "source": [
    "# lets derivate loss(Categorical Cross Entropy) with respect to the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2571cf30-9657-4bb5-8508-7e001ff38528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(dvalues,y_true):\n",
    "    samples=len(dvalues)\n",
    "    \n",
    "    #elements of y_true\n",
    "    element=len(dvalues[0])\n",
    "\n",
    "    #lets chake the shape\n",
    "    if len(y_true.shape)==1:\n",
    "        y_true=y_true.T\n",
    "        y_true=np.diagflat(y_true)   #making eke\n",
    "\n",
    "    print(\"hello from inside function\")\n",
    "    print(y_true)\n",
    "    dinputs=-y_true/dvalues\n",
    "    #lets normalize the gradient\n",
    "    dinputs=dinputs/samples\n",
    "    return dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db1059f3-163b-4eea-92cf-e1286078aca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d207a7f5-9436-4499-8dc8-52c58ff7700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvalues=np.random.randn(3,2)\n",
    "y_true=np.array([[1,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "562db81a-33ce-4e11-b2a9-7f9824ee1cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2) (1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(dvalues.shape,y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1e3b4-90b0-4ffd-b74e-475551765e15",
   "metadata": {},
   "source": [
    "# lets implment the softmax derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109784ed-e757-427c-adfb-f48c38eec141",
   "metadata": {},
   "source": [
    "* the Equation for the softmax derivative looks like this:  \\\n",
    " derivative= Sij*QJK-Sij*Sik   ** here the QJK stands for the kronecker delta which has the following values \\\n",
    "                      Sij  = 0 ==> if i=j\\\n",
    "                           = 1 ==> if they are not equal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b34c45-a5c5-4f3c-a393-f680e8da2460",
   "metadata": {},
   "source": [
    "## left side of the derivative formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44a1d858-5023-40a5-9cd7-d63b52f7dd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7]\n",
      " [0.1]\n",
      " [0.2]]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs=np.array([[0.7,0.1,0.2]]).T\n",
    "print(softmax_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fb3c070-8c27-4c5d-90b1-3819582ff6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "square_shaped=np.eye(softmax_outputs.shape[0])\n",
    "print(square_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "feb4d643-53d1-4b47-bedf-a4f0520d96d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs.shape)\n",
    "print(square_shaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbb7202f-0293-49c3-9fcb-ebd28603f844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7, 0. , 0. ],\n",
       "       [0. , 0.1, 0. ],\n",
       "       [0. , 0. , 0.2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_outputs*square_shaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04faa4b9-29e9-4a42-bf15-00964aa84470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try using diagflat... my experiment with diagflat\n",
    "\n",
    "# temp1=np.array([[1,2,3,4]])\n",
    "# np.diagflat(temp1)\n",
    "\n",
    "#lets try using column vector\n",
    "# temp2=np.array([1,2,3,4]).reshape(-1,1)\n",
    "# np.diagflat(temp2)\n",
    "\n",
    "#lets try it on matrix format\n",
    "# p=np.array([\n",
    "#     [1,1,1,1],\n",
    "#     [2,2,2,2],\n",
    "#     [3,3,3,3]\n",
    "# ])\n",
    "# np.diagflat(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cae874-4c5f-4df1-9bc1-a4bca1cf56dd",
   "metadata": {},
   "source": [
    "## right side of the derivative formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afa9f304-ae77-41c3-a9ca-6e4d7e6f6565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49, 0.07, 0.14],\n",
       "       [0.07, 0.01, 0.02],\n",
       "       [0.14, 0.02, 0.04]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(softmax_outputs,softmax_outputs.T)  #the result is called jacbobias matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9641f3f-bae8-4a97-8ea5-d0d9602e97e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1=np.array([1,2,3])\n",
    "temp2=np.array([4,5,6])\n",
    "np.dot(temp1,temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4928bce-faf3-4320-83e5-c5b24a69694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets implemnen the backprop for the softmax\n",
    "class Softmax:\n",
    "    def forward(self,inputs,weights):\n",
    "        self.outputs=np.dot(inputs,weights.T)\n",
    "        \n",
    "    def backward(self,dvalues):\n",
    "        self.dinputs=np.zeros_like(dvalues)\n",
    "        for index,(ind_outputs,ind_dvalues) in enumerate(zip(self.outputs,dvalues)):\n",
    "            #making the individual output column vector\n",
    "            ind_outputs=ind_outputs.reshape(-1,1)\n",
    "            left_side=np.diagfiat(ind_outputs)\n",
    "            right_side=np.dot(ind_outputs,ind_outputs.T)\n",
    "            jackobian_matrix=left_side-right_side\n",
    "            self.dinputs[index]=np.dot(ind_dvalues,jackobian_matrix)\n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03c0314d-a962-401c-985a-40426999761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp1=np.random.randn(3,3)\n",
    "# temp2=np.random.randn(1,3)\n",
    "# np.dot(temp1,temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2447b001-8849-44fc-9599-e5405dae4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation import Softmax\n",
    "softmax_fn=Softmax()\n",
    "inputs=np.random.randn(3,4)\n",
    "softmax_fn.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d24064f4-a10a-437a-928c-d6c7a0b1fed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26837355, 0.27983664, 0.1968649 , 0.25492491],\n",
       "       [0.36366096, 0.2847154 , 0.18851472, 0.16310892],\n",
       "       [0.08648717, 0.0906067 , 0.55879017, 0.26411596]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_fn.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f0869-c223-478a-9b22-02c86f29cd37",
   "metadata": {},
   "source": [
    "# common categorical cross entropy loss and softmax activation derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffec3c-68d5-4506-9625-592743bef2ca",
   "metadata": {},
   "source": [
    "* we are going to extract the dervivative of categorical corss entropy from softmax activation function\n",
    "* its simpler and faster to comupte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2c5fdf9-673d-4a95-89e9-eeb45c284dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation import Softmax\n",
    "from loss import Categorical_loss_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1171188c-3552-48a9-a2d2-e505d8f642b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_cross_entropy_loss:\n",
    "    def __init__(self):\n",
    "        self.activation=Softmax()\n",
    "        self.cross_entropy_loss=Categorical_loss_entropy()\n",
    "        \n",
    "    def forward(self,inputs,y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.outputs=self.activation.outputs\n",
    "        return self.loss.calculate(self.outputs,y_true)  #the average loss\n",
    "        \n",
    "    def backward(self,dvalues,y_true):\n",
    "        \"\"\"deriving the loss with repsect to the inputs(outputs of softmax).\"\"\"\n",
    "        self.dinputs=dvalues.copy()\n",
    "        samples=len(dvalues)\n",
    "        \n",
    "        #check if the y_true is one hot encoded \n",
    "        if len(y_true.shape)==2:\n",
    "            y_true=np.argmax(y_true,axis=1)\n",
    "\n",
    "        self.dinputs[range(samples),y_true]-=1   #substract one from the value of the true class\n",
    "\n",
    "        #lets normalize for the inputs\n",
    "        self.dinputs=self.dinputs/samples\n",
    "        #return self.dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10b5d2-4c9e-4912-b800-f51d1cefebcd",
   "metadata": {},
   "source": [
    "## Test Test Test between combined calculation of the activation and loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ae3c3b2-a8a1-434a-89a9-bfa6e8257e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n"
     ]
    }
   ],
   "source": [
    "#seeding\n",
    "#nnfs.init()\n",
    "#seperate calculation of the softmax and loss\n",
    "activation=Softmax()\n",
    "loss=Categorical_loss_entropy()\n",
    "\n",
    "softmax_outputs=np.array([\n",
    "    [0.7,0.1,0.2],\n",
    "    [0.1,0.5,0.4],\n",
    "    [0.02,0.9,0.08]\n",
    "])    #output for each inputs this is a batch of 3 inputs\n",
    "\n",
    "true_values=np.array([0,1,1])\n",
    "\n",
    "activation.outputs=softmax_outputs\n",
    "#calculate the backward pass\n",
    "loss.backward(softmax_outputs,true_values)\n",
    "activation.backward(loss.dinputs)\n",
    "dinputs1=activation.dinputs\n",
    "print(dinputs1)\n",
    "\n",
    "#all in one operation\n",
    "softmax_cross_entropy=softmax_cross_entropy_loss()\n",
    "softmax_cross_entropy.backward(softmax_outputs,true_values)\n",
    "dinputs2=softmax_cross_entropy.dinputs\n",
    "print(dinputs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b336d8ed-7ebe-4ffc-bfb2-3ea1f69f408e",
   "metadata": {},
   "source": [
    "## Time it..timing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d720ce8-e75b-4bbe-8ff2-16049378a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e95da25-8d49-4e09-9aac-0ab70652363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first time:0.0005750999999998285\n",
      "the second time:0.0006941999999998671\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs=np.array([\n",
    "    [0.7,0.2,0.1],\n",
    "    [0.1,0.5,0.4],\n",
    "    [0.02,0.9,0.08]\n",
    "])    #output for each inputs this is a batch of 3 inputs\n",
    "\n",
    "true_values=np.array([0,1,1])\n",
    "\n",
    "## lets time it \n",
    "def seperate_calculation():\n",
    "    activation_fn=Softmax()\n",
    "    loss=Categorical_loss_entropy()\n",
    "    \n",
    "    activation_fn.outputs=softmax_outputs\n",
    "    #backpropagation\n",
    "    loss.backward(softmax_outputs,true_values)\n",
    "    activation.backward(loss.dinputs)\n",
    "    dniputs=activation.dinputs\n",
    "\n",
    "def all_in_one():\n",
    "    act_loss=softmax_cross_entropy_loss()\n",
    "    act_loss.backward(softmax_outputs,true_values)\n",
    "    dinput2-act_loss.dinputs\n",
    "\n",
    "#lets start the timing\n",
    "t1=timeit(lambda:seperate_calculation,number=10000)\n",
    "t2=timeit(lambda:all_in_one,number=10000)\n",
    "\n",
    "print(f\"the first time:{t1}\")\n",
    "print(f\"the second time:{t2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f062ca-dde7-478e-b1d0-4da62c09f517",
   "metadata": {},
   "source": [
    "# Full code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24d8c8ae-4c3e-46f7-b367-dbce18939d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "from dense import Dense\n",
    "from activation import Relu\n",
    "from loss import softmax_categorical_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f8f60f6-c4cc-4697-89d2-26e4cdce79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333332 0.33333334 0.33333334]\n",
      " [0.33333327 0.33333336 0.33333337]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333276 0.3333334  0.33333384]]\n",
      "the accuracys:0.3200\n",
      "[[ 3.44759462e-06  5.04246346e-06  4.99954418e-06]\n",
      " [ 3.44759467e-06  5.04246355e-06  4.99954425e-06]\n",
      " [ 3.44759493e-06  5.04246390e-06  4.99954465e-06]\n",
      " ...\n",
      " [-3.10018578e-06 -1.77329922e-06 -5.61497134e-06]\n",
      " [-3.10018578e-06 -1.77329922e-06 -5.61497134e-06]\n",
      " [-3.10018578e-06 -1.77329922e-06 -5.61497134e-06]]\n",
      "[[-1.49657517e-04  1.26060368e-04  2.35971488e-05]\n",
      " [-2.19338525e-05 -2.52707366e-05  4.72045891e-05]\n",
      " [-1.26051655e-04  1.12613191e-04  1.34384646e-05]]\n",
      "[[-2.38394806e-05  2.60734618e-06  2.12321344e-05]]\n",
      "[[ 0.00000000e+00  0.00000000e+00]\n",
      " [ 2.72782623e-08  1.21979042e-08]\n",
      " [ 1.45923566e-07 -1.12819103e-08]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00]]\n",
      "[[ 1.11579627e-04  1.59562228e-04  1.54700764e-04]\n",
      " [-4.27182840e-05 -2.18310999e-04  2.07406047e-05]]\n",
      "[[0.00029757 0.00051302 0.00032243]]\n"
     ]
    }
   ],
   "source": [
    "#define the architechtire\n",
    "dense1=Dense(2,3)\n",
    "activation=Relu()\n",
    "dense2=Dense(3,3)\n",
    "softmax_loss=softmax_categorical_loss()\n",
    "\n",
    "#lets get the data\n",
    "x,y=spiral_data(samples=1000,classes=3)\n",
    "\n",
    "#the forward pass of the neural network\n",
    "dense1.forward(x)\n",
    "activation.forward(dense1.outputs)\n",
    "dense2.forward(activation.output)\n",
    "\n",
    "loss=softmax_loss.forward(dense2.outputs,y)\n",
    "\n",
    "print(softmax_loss.outputs[:5])\n",
    "#print(loss_v)\n",
    "\n",
    "#lets check for the accuracy\n",
    "prediction=np.argmax(softmax_loss.outputs,axis=1)\n",
    "if len(y.shape)==2:\n",
    "    y=np.argmax(y,axis=1)\n",
    "    \n",
    "accuracy=np.mean(prediction==y)\n",
    "print(f\"the accuracys:{accuracy:.4f}\")\n",
    "\n",
    "#lets start the backprop\n",
    "softmax_loss.backward(softmax_loss.outputs,y)\n",
    "dense2.backward(softmax_loss.dinputs)\n",
    "activation.backward(dense2.dinputs)\n",
    "dense1.backward(activation.drelu)\n",
    "\n",
    "#lets print some values here\n",
    "print(dense2.dinputs)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbias)\n",
    "\n",
    "print(dense1.dinputs)\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28157b5f-7129-4bf3-9ad4-ce629823b416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

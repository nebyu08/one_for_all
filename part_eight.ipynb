{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddd8684-04e7-477b-a7f7-c598b7b6b725",
   "metadata": {},
   "source": [
    "### for dynamically accessing the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c63fd0-5d36-4804-866c-932810644c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4111d32b-5fb2-40c0-8e44-0c95f73884a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dense import Dense\n",
    "from optimization import SGD,Ada_Grad,RMS_Prop,Adam\n",
    "from activation import Relu\n",
    "from loss import softmax_categorical_loss\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0af1dd9-1334-4d95-8652-dd28114c1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=spiral_data(samples=1000,classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191ecbb4-2793-4290-8673-669c52e90b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss:1.10\n",
      "the accuracy is 0.27\n"
     ]
    }
   ],
   "source": [
    "#lets build a neural network model\n",
    "\n",
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "#lets see the forward pass of the model\n",
    "layer1.forward(x)\n",
    "\n",
    "act1.forward(layer1.outputs)\n",
    "\n",
    "layer2.forward(act1.output)\n",
    "\n",
    "loss=act_loss.forward(layer2.outputs,y)\n",
    "\n",
    "#lets define the optimizer for the model\n",
    "optimizer=SGD()\n",
    "\n",
    "#lets display the loss of the model\n",
    "print(f\"the loss:{loss:.2f}\")\n",
    "\n",
    "#lets calculate the accuracy of the model\n",
    "prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "\n",
    "if len(y.shape)==2:\n",
    "    y=argmax(y,axis=1)\n",
    "\n",
    "accuracy=np.mean(prediction==y)\n",
    "\n",
    "#the accuracy of the model is\n",
    "print(f\"the accuracy is {accuracy:.2f}\")\n",
    "\n",
    "#lets backward pass\n",
    "act_loss.backward(act_loss.outputs,y)\n",
    "layer2.backward(act_loss.dinputs)\n",
    "act1.backward(layer2.dinputs)\n",
    "layer1.backward(act1.drelu)\n",
    "\n",
    "#lets optimize the model\n",
    "optimizer.update_params(layer1)\n",
    "optimizer.update_params(layer2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76de0dd-0c89-46c5-97da-14811b37b910",
   "metadata": {},
   "source": [
    "# lets loop the training and optimizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb0f9c4-0c1f-42e6-8cf6-c920c09bc24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the epoch 0,loss: 1.10 , accuracy:0.38\n",
      "the epoch 1000,loss: 1.10 , accuracy:0.33\n",
      "the epoch 2000,loss: 1.10 , accuracy:0.33\n",
      "the epoch 3000,loss: 1.67 , accuracy:0.33\n",
      "the epoch 4000,loss: 10.75 , accuracy:0.33\n",
      "the epoch 5000,loss: 10.75 , accuracy:0.33\n",
      "the epoch 6000,loss: 10.75 , accuracy:0.33\n",
      "the epoch 7000,loss: 10.75 , accuracy:0.33\n",
      "the epoch 8000,loss: 10.75 , accuracy:0.33\n",
      "the epoch 9000,loss: 10.75 , accuracy:0.33\n"
     ]
    }
   ],
   "source": [
    "#lets define the model architecture\n",
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "epoch=10000\n",
    "for i in range(epoch):\n",
    "    layer1.forward(x)\n",
    "\n",
    "    act1.forward(layer1.outputs)\n",
    "    \n",
    "    layer2.forward(act1.output)\n",
    "    \n",
    "    loss=act_loss.forward(layer2.outputs,y)\n",
    "\n",
    "    #lets display the loss of the model\n",
    "    #print(f\"the loss:{loss:.2f}\")\n",
    "    \n",
    "    #lets calculate the accuracy of the model\n",
    "    prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "    \n",
    "    if len(y.shape)==2:\n",
    "        y=argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(prediction==y)\n",
    "    \n",
    "    #the accuracy of the model is\n",
    "    #print(f\"the accuracy is {accuracy:.2f}\")\n",
    "    if i%1000==0:\n",
    "        print(f\"the epoch {i},loss: {loss:.2f} , accuracy:{accuracy:.2f}\")\n",
    "    #lets backward pass\n",
    "    act_loss.backward(act_loss.outputs,y)\n",
    "    layer2.backward(act_loss.dinputs)\n",
    "    act1.backward(layer2.dinputs)\n",
    "    layer1.backward(act1.drelu)\n",
    "    \n",
    "    #lets optimize the model\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5dd61-626c-41ef-be9a-0347098ff5e3",
   "metadata": {},
   "source": [
    "# lets change the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ab9881f-0b6e-46ed-8675-a712a7eabae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets make the architechture of the model\n",
    "x,y=spiral_data(samples=100,classes=3)\n",
    "dense1=Dense(2,64)\n",
    "act1=Relu()\n",
    "dense2=Dense(64,3)\n",
    "loss=softmax_categorical_loss()\n",
    "optimizer=SGD(lr=0.8) #the learning rate is 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18ddf296-44e4-4d48-bc13-0e6a0e5a756a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accury is:0.33,he loss value is nan\n",
      "the accury is:0.33,he loss value is nan\n",
      "the accury is:0.33,he loss value is nan\n",
      "the accury is:0.33,he loss value is nan\n",
      "the accury is:0.33,he loss value is nan\n",
      "the accury is:0.33,he loss value is nan\n",
      "the accury is:0.33,he loss value is nan\n",
      "the accury is:0.33,he loss value is nan\n",
      "the accury is:0.33,he loss value is nan\n",
      "the accury is:0.33,he loss value is nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    #forward propagation\n",
    "    dense1.forward(x)\n",
    "    act1.forward(dense1.outputs)\n",
    "    dense2.forward(act1.output)\n",
    "    loss_val=loss.forward(dense2.outputs,y)\n",
    " \n",
    "    #lets make the prediction into one dimension\n",
    "    y_pred=np.argmax(loss.outputs,axis=1)\n",
    "    \n",
    "    #the accuracy \n",
    "    if len(y.shape)==2:\n",
    "        y=np.argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(y_pred==y)\n",
    "\n",
    "    #lets print the process of the model training\n",
    "    if (i%1000==0):\n",
    "        print(f\"the accury is:{accuracy:.2f},he loss value is {loss_val}\")\n",
    "    \n",
    "    #lets backpropagate\n",
    "    loss.backward(loss.outputs,y)\n",
    "    dense2.backward(loss.dinputs)\n",
    "    act1.backward(dense2.dinputs)\n",
    "    dense1.backward(act1.drelu)\n",
    "\n",
    "    #lets update the parameter of the model\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a3d36f4-37ac-4aee-b7bf-59021db0d960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.337, loss: 1.099\n",
      "epoch: 100, acc: 0.333, loss: 10.745\n",
      "epoch: 200, acc: 0.333, loss: 10.745\n",
      "epoch: 300, acc: 0.333, loss: 10.745\n",
      "epoch: 400, acc: 0.333, loss: 10.745\n",
      "epoch: 500, acc: 0.333, loss: 10.745\n",
      "epoch: 600, acc: 0.333, loss: 10.745\n",
      "epoch: 700, acc: 0.333, loss: 10.745\n",
      "epoch: 800, acc: 0.333, loss: nan\n",
      "epoch: 900, acc: 0.333, loss: nan\n",
      "epoch: 1000, acc: 0.333, loss: nan\n",
      "epoch: 1100, acc: 0.333, loss: nan\n",
      "epoch: 1200, acc: 0.333, loss: nan\n",
      "epoch: 1300, acc: 0.333, loss: nan\n",
      "epoch: 1400, acc: 0.333, loss: nan\n",
      "epoch: 1500, acc: 0.333, loss: nan\n",
      "epoch: 1600, acc: 0.333, loss: nan\n",
      "epoch: 1700, acc: 0.333, loss: nan\n",
      "epoch: 1800, acc: 0.333, loss: nan\n",
      "epoch: 1900, acc: 0.333, loss: nan\n",
      "epoch: 2000, acc: 0.333, loss: nan\n",
      "epoch: 2100, acc: 0.333, loss: nan\n",
      "epoch: 2200, acc: 0.333, loss: nan\n",
      "epoch: 2300, acc: 0.333, loss: nan\n",
      "epoch: 2400, acc: 0.333, loss: nan\n",
      "epoch: 2500, acc: 0.333, loss: nan\n",
      "epoch: 2600, acc: 0.333, loss: nan\n",
      "epoch: 2700, acc: 0.333, loss: nan\n",
      "epoch: 2800, acc: 0.333, loss: nan\n",
      "epoch: 2900, acc: 0.333, loss: nan\n",
      "epoch: 3000, acc: 0.333, loss: nan\n",
      "epoch: 3100, acc: 0.333, loss: nan\n",
      "epoch: 3200, acc: 0.333, loss: nan\n",
      "epoch: 3300, acc: 0.333, loss: nan\n",
      "epoch: 3400, acc: 0.333, loss: nan\n",
      "epoch: 3500, acc: 0.333, loss: nan\n",
      "epoch: 3600, acc: 0.333, loss: nan\n",
      "epoch: 3700, acc: 0.333, loss: nan\n",
      "epoch: 3800, acc: 0.333, loss: nan\n",
      "epoch: 3900, acc: 0.333, loss: nan\n",
      "epoch: 4000, acc: 0.333, loss: nan\n",
      "epoch: 4100, acc: 0.333, loss: nan\n",
      "epoch: 4200, acc: 0.333, loss: nan\n",
      "epoch: 4300, acc: 0.333, loss: nan\n",
      "epoch: 4400, acc: 0.333, loss: nan\n",
      "epoch: 4500, acc: 0.333, loss: nan\n",
      "epoch: 4600, acc: 0.333, loss: nan\n",
      "epoch: 4700, acc: 0.333, loss: nan\n",
      "epoch: 4800, acc: 0.333, loss: nan\n",
      "epoch: 4900, acc: 0.333, loss: nan\n",
      "epoch: 5000, acc: 0.333, loss: nan\n",
      "epoch: 5100, acc: 0.333, loss: nan\n",
      "epoch: 5200, acc: 0.333, loss: nan\n",
      "epoch: 5300, acc: 0.333, loss: nan\n",
      "epoch: 5400, acc: 0.333, loss: nan\n",
      "epoch: 5500, acc: 0.333, loss: nan\n",
      "epoch: 5600, acc: 0.333, loss: nan\n",
      "epoch: 5700, acc: 0.333, loss: nan\n",
      "epoch: 5800, acc: 0.333, loss: nan\n",
      "epoch: 5900, acc: 0.333, loss: nan\n",
      "epoch: 6000, acc: 0.333, loss: nan\n",
      "epoch: 6100, acc: 0.333, loss: nan\n",
      "epoch: 6200, acc: 0.333, loss: nan\n",
      "epoch: 6300, acc: 0.333, loss: nan\n",
      "epoch: 6400, acc: 0.333, loss: nan\n",
      "epoch: 6500, acc: 0.333, loss: nan\n",
      "epoch: 6600, acc: 0.333, loss: nan\n",
      "epoch: 6700, acc: 0.333, loss: nan\n",
      "epoch: 6800, acc: 0.333, loss: nan\n",
      "epoch: 6900, acc: 0.333, loss: nan\n",
      "epoch: 7000, acc: 0.333, loss: nan\n",
      "epoch: 7100, acc: 0.333, loss: nan\n",
      "epoch: 7200, acc: 0.333, loss: nan\n",
      "epoch: 7300, acc: 0.333, loss: nan\n",
      "epoch: 7400, acc: 0.333, loss: nan\n",
      "epoch: 7500, acc: 0.333, loss: nan\n",
      "epoch: 7600, acc: 0.333, loss: nan\n",
      "epoch: 7700, acc: 0.333, loss: nan\n",
      "epoch: 7800, acc: 0.333, loss: nan\n",
      "epoch: 7900, acc: 0.333, loss: nan\n",
      "epoch: 8000, acc: 0.333, loss: nan\n",
      "epoch: 8100, acc: 0.333, loss: nan\n",
      "epoch: 8200, acc: 0.333, loss: nan\n",
      "epoch: 8300, acc: 0.333, loss: nan\n",
      "epoch: 8400, acc: 0.333, loss: nan\n",
      "epoch: 8500, acc: 0.333, loss: nan\n",
      "epoch: 8600, acc: 0.333, loss: nan\n",
      "epoch: 8700, acc: 0.333, loss: nan\n",
      "epoch: 8800, acc: 0.333, loss: nan\n",
      "epoch: 8900, acc: 0.333, loss: nan\n",
      "epoch: 9000, acc: 0.333, loss: nan\n",
      "epoch: 9100, acc: 0.333, loss: nan\n",
      "epoch: 9200, acc: 0.333, loss: nan\n",
      "epoch: 9300, acc: 0.333, loss: nan\n",
      "epoch: 9400, acc: 0.333, loss: nan\n",
      "epoch: 9500, acc: 0.333, loss: nan\n",
      "epoch: 9600, acc: 0.333, loss: nan\n",
      "epoch: 9700, acc: 0.333, loss: nan\n",
      "epoch: 9800, acc: 0.333, loss: nan\n",
      "epoch: 9900, acc: 0.333, loss: nan\n",
      "epoch: 10000, acc: 0.333, loss: nan\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Relu()\n",
    "\n",
    "dense2 = Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = softmax_categorical_loss()\n",
    "# Create optimizer\n",
    "optimizer = SGD(lr=.85)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "# Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.outputs)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.outputs, y)\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.outputs, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}')\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.outputs, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.drelu)\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec93042-307f-4f49-8e05-2147788910aa",
   "metadata": {},
   "source": [
    "## playing around with learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2e59c4c-85f6-40cc-bac6-97f91c9193de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step is 0 and learning rate is 1.000 \n",
      "step is 1 and learning rate is 0.909 \n",
      "step is 2 and learning rate is 0.833 \n",
      "step is 3 and learning rate is 0.769 \n",
      "step is 4 and learning rate is 0.714 \n",
      "step is 5 and learning rate is 0.667 \n",
      "step is 6 and learning rate is 0.625 \n",
      "step is 7 and learning rate is 0.588 \n",
      "step is 8 and learning rate is 0.556 \n",
      "step is 9 and learning rate is 0.526 \n",
      "step is 10 and learning rate is 0.500 \n",
      "step is 11 and learning rate is 0.476 \n",
      "step is 12 and learning rate is 0.455 \n",
      "step is 13 and learning rate is 0.435 \n",
      "step is 14 and learning rate is 0.417 \n",
      "step is 15 and learning rate is 0.400 \n",
      "step is 16 and learning rate is 0.385 \n",
      "step is 17 and learning rate is 0.370 \n",
      "step is 18 and learning rate is 0.357 \n",
      "step is 19 and learning rate is 0.345 \n",
      "step is 20 and learning rate is 0.333 \n",
      "step is 21 and learning rate is 0.323 \n",
      "step is 22 and learning rate is 0.312 \n",
      "step is 23 and learning rate is 0.303 \n",
      "step is 24 and learning rate is 0.294 \n",
      "step is 25 and learning rate is 0.286 \n",
      "step is 26 and learning rate is 0.278 \n",
      "step is 27 and learning rate is 0.270 \n",
      "step is 28 and learning rate is 0.263 \n",
      "step is 29 and learning rate is 0.256 \n",
      "step is 30 and learning rate is 0.250 \n",
      "step is 31 and learning rate is 0.244 \n",
      "step is 32 and learning rate is 0.238 \n",
      "step is 33 and learning rate is 0.233 \n",
      "step is 34 and learning rate is 0.227 \n",
      "step is 35 and learning rate is 0.222 \n",
      "step is 36 and learning rate is 0.217 \n",
      "step is 37 and learning rate is 0.213 \n",
      "step is 38 and learning rate is 0.208 \n",
      "step is 39 and learning rate is 0.204 \n",
      "step is 40 and learning rate is 0.200 \n",
      "step is 41 and learning rate is 0.196 \n",
      "step is 42 and learning rate is 0.192 \n",
      "step is 43 and learning rate is 0.189 \n",
      "step is 44 and learning rate is 0.185 \n",
      "step is 45 and learning rate is 0.182 \n",
      "step is 46 and learning rate is 0.179 \n",
      "step is 47 and learning rate is 0.175 \n",
      "step is 48 and learning rate is 0.172 \n",
      "step is 49 and learning rate is 0.169 \n",
      "step is 50 and learning rate is 0.167 \n",
      "step is 51 and learning rate is 0.164 \n",
      "step is 52 and learning rate is 0.161 \n",
      "step is 53 and learning rate is 0.159 \n",
      "step is 54 and learning rate is 0.156 \n",
      "step is 55 and learning rate is 0.154 \n",
      "step is 56 and learning rate is 0.152 \n",
      "step is 57 and learning rate is 0.149 \n",
      "step is 58 and learning rate is 0.147 \n",
      "step is 59 and learning rate is 0.145 \n",
      "step is 60 and learning rate is 0.143 \n",
      "step is 61 and learning rate is 0.141 \n",
      "step is 62 and learning rate is 0.139 \n",
      "step is 63 and learning rate is 0.137 \n",
      "step is 64 and learning rate is 0.135 \n",
      "step is 65 and learning rate is 0.133 \n",
      "step is 66 and learning rate is 0.132 \n",
      "step is 67 and learning rate is 0.130 \n",
      "step is 68 and learning rate is 0.128 \n",
      "step is 69 and learning rate is 0.127 \n",
      "step is 70 and learning rate is 0.125 \n",
      "step is 71 and learning rate is 0.123 \n",
      "step is 72 and learning rate is 0.122 \n",
      "step is 73 and learning rate is 0.120 \n",
      "step is 74 and learning rate is 0.119 \n",
      "step is 75 and learning rate is 0.118 \n",
      "step is 76 and learning rate is 0.116 \n",
      "step is 77 and learning rate is 0.115 \n",
      "step is 78 and learning rate is 0.114 \n",
      "step is 79 and learning rate is 0.112 \n",
      "step is 80 and learning rate is 0.111 \n",
      "step is 81 and learning rate is 0.110 \n",
      "step is 82 and learning rate is 0.109 \n",
      "step is 83 and learning rate is 0.108 \n",
      "step is 84 and learning rate is 0.106 \n",
      "step is 85 and learning rate is 0.105 \n",
      "step is 86 and learning rate is 0.104 \n",
      "step is 87 and learning rate is 0.103 \n",
      "step is 88 and learning rate is 0.102 \n",
      "step is 89 and learning rate is 0.101 \n",
      "step is 90 and learning rate is 0.100 \n",
      "step is 91 and learning rate is 0.099 \n",
      "step is 92 and learning rate is 0.098 \n",
      "step is 93 and learning rate is 0.097 \n",
      "step is 94 and learning rate is 0.096 \n",
      "step is 95 and learning rate is 0.095 \n",
      "step is 96 and learning rate is 0.094 \n",
      "step is 97 and learning rate is 0.093 \n",
      "step is 98 and learning rate is 0.093 \n",
      "step is 99 and learning rate is 0.092 \n",
      "step is 100 and learning rate is 0.091 \n",
      "step is 101 and learning rate is 0.090 \n",
      "step is 102 and learning rate is 0.089 \n",
      "step is 103 and learning rate is 0.088 \n",
      "step is 104 and learning rate is 0.088 \n",
      "step is 105 and learning rate is 0.087 \n",
      "step is 106 and learning rate is 0.086 \n",
      "step is 107 and learning rate is 0.085 \n",
      "step is 108 and learning rate is 0.085 \n",
      "step is 109 and learning rate is 0.084 \n",
      "step is 110 and learning rate is 0.083 \n",
      "step is 111 and learning rate is 0.083 \n",
      "step is 112 and learning rate is 0.082 \n",
      "step is 113 and learning rate is 0.081 \n",
      "step is 114 and learning rate is 0.081 \n",
      "step is 115 and learning rate is 0.080 \n",
      "step is 116 and learning rate is 0.079 \n",
      "step is 117 and learning rate is 0.079 \n",
      "step is 118 and learning rate is 0.078 \n",
      "step is 119 and learning rate is 0.078 \n",
      "step is 120 and learning rate is 0.077 \n",
      "step is 121 and learning rate is 0.076 \n",
      "step is 122 and learning rate is 0.076 \n",
      "step is 123 and learning rate is 0.075 \n",
      "step is 124 and learning rate is 0.075 \n",
      "step is 125 and learning rate is 0.074 \n",
      "step is 126 and learning rate is 0.074 \n",
      "step is 127 and learning rate is 0.073 \n",
      "step is 128 and learning rate is 0.072 \n",
      "step is 129 and learning rate is 0.072 \n",
      "step is 130 and learning rate is 0.071 \n",
      "step is 131 and learning rate is 0.071 \n",
      "step is 132 and learning rate is 0.070 \n",
      "step is 133 and learning rate is 0.070 \n",
      "step is 134 and learning rate is 0.069 \n",
      "step is 135 and learning rate is 0.069 \n",
      "step is 136 and learning rate is 0.068 \n",
      "step is 137 and learning rate is 0.068 \n",
      "step is 138 and learning rate is 0.068 \n",
      "step is 139 and learning rate is 0.067 \n",
      "step is 140 and learning rate is 0.067 \n",
      "step is 141 and learning rate is 0.066 \n",
      "step is 142 and learning rate is 0.066 \n",
      "step is 143 and learning rate is 0.065 \n",
      "step is 144 and learning rate is 0.065 \n",
      "step is 145 and learning rate is 0.065 \n",
      "step is 146 and learning rate is 0.064 \n",
      "step is 147 and learning rate is 0.064 \n",
      "step is 148 and learning rate is 0.063 \n",
      "step is 149 and learning rate is 0.063 \n",
      "step is 150 and learning rate is 0.062 \n",
      "step is 151 and learning rate is 0.062 \n",
      "step is 152 and learning rate is 0.062 \n",
      "step is 153 and learning rate is 0.061 \n",
      "step is 154 and learning rate is 0.061 \n",
      "step is 155 and learning rate is 0.061 \n",
      "step is 156 and learning rate is 0.060 \n",
      "step is 157 and learning rate is 0.060 \n",
      "step is 158 and learning rate is 0.060 \n",
      "step is 159 and learning rate is 0.059 \n",
      "step is 160 and learning rate is 0.059 \n",
      "step is 161 and learning rate is 0.058 \n",
      "step is 162 and learning rate is 0.058 \n",
      "step is 163 and learning rate is 0.058 \n",
      "step is 164 and learning rate is 0.057 \n",
      "step is 165 and learning rate is 0.057 \n",
      "step is 166 and learning rate is 0.057 \n",
      "step is 167 and learning rate is 0.056 \n",
      "step is 168 and learning rate is 0.056 \n",
      "step is 169 and learning rate is 0.056 \n",
      "step is 170 and learning rate is 0.056 \n",
      "step is 171 and learning rate is 0.055 \n",
      "step is 172 and learning rate is 0.055 \n",
      "step is 173 and learning rate is 0.055 \n",
      "step is 174 and learning rate is 0.054 \n",
      "step is 175 and learning rate is 0.054 \n",
      "step is 176 and learning rate is 0.054 \n",
      "step is 177 and learning rate is 0.053 \n",
      "step is 178 and learning rate is 0.053 \n",
      "step is 179 and learning rate is 0.053 \n",
      "step is 180 and learning rate is 0.053 \n",
      "step is 181 and learning rate is 0.052 \n",
      "step is 182 and learning rate is 0.052 \n",
      "step is 183 and learning rate is 0.052 \n",
      "step is 184 and learning rate is 0.052 \n",
      "step is 185 and learning rate is 0.051 \n",
      "step is 186 and learning rate is 0.051 \n",
      "step is 187 and learning rate is 0.051 \n",
      "step is 188 and learning rate is 0.051 \n",
      "step is 189 and learning rate is 0.050 \n",
      "step is 190 and learning rate is 0.050 \n",
      "step is 191 and learning rate is 0.050 \n",
      "step is 192 and learning rate is 0.050 \n",
      "step is 193 and learning rate is 0.049 \n",
      "step is 194 and learning rate is 0.049 \n",
      "step is 195 and learning rate is 0.049 \n",
      "step is 196 and learning rate is 0.049 \n",
      "step is 197 and learning rate is 0.048 \n",
      "step is 198 and learning rate is 0.048 \n",
      "step is 199 and learning rate is 0.048 \n",
      "step is 200 and learning rate is 0.048 \n",
      "step is 201 and learning rate is 0.047 \n",
      "step is 202 and learning rate is 0.047 \n",
      "step is 203 and learning rate is 0.047 \n",
      "step is 204 and learning rate is 0.047 \n",
      "step is 205 and learning rate is 0.047 \n",
      "step is 206 and learning rate is 0.046 \n",
      "step is 207 and learning rate is 0.046 \n",
      "step is 208 and learning rate is 0.046 \n",
      "step is 209 and learning rate is 0.046 \n",
      "step is 210 and learning rate is 0.045 \n",
      "step is 211 and learning rate is 0.045 \n",
      "step is 212 and learning rate is 0.045 \n",
      "step is 213 and learning rate is 0.045 \n",
      "step is 214 and learning rate is 0.045 \n",
      "step is 215 and learning rate is 0.044 \n",
      "step is 216 and learning rate is 0.044 \n",
      "step is 217 and learning rate is 0.044 \n",
      "step is 218 and learning rate is 0.044 \n",
      "step is 219 and learning rate is 0.044 \n",
      "step is 220 and learning rate is 0.043 \n",
      "step is 221 and learning rate is 0.043 \n",
      "step is 222 and learning rate is 0.043 \n",
      "step is 223 and learning rate is 0.043 \n",
      "step is 224 and learning rate is 0.043 \n",
      "step is 225 and learning rate is 0.043 \n",
      "step is 226 and learning rate is 0.042 \n",
      "step is 227 and learning rate is 0.042 \n",
      "step is 228 and learning rate is 0.042 \n",
      "step is 229 and learning rate is 0.042 \n",
      "step is 230 and learning rate is 0.042 \n",
      "step is 231 and learning rate is 0.041 \n",
      "step is 232 and learning rate is 0.041 \n",
      "step is 233 and learning rate is 0.041 \n",
      "step is 234 and learning rate is 0.041 \n",
      "step is 235 and learning rate is 0.041 \n",
      "step is 236 and learning rate is 0.041 \n",
      "step is 237 and learning rate is 0.040 \n",
      "step is 238 and learning rate is 0.040 \n",
      "step is 239 and learning rate is 0.040 \n",
      "step is 240 and learning rate is 0.040 \n",
      "step is 241 and learning rate is 0.040 \n",
      "step is 242 and learning rate is 0.040 \n",
      "step is 243 and learning rate is 0.040 \n",
      "step is 244 and learning rate is 0.039 \n",
      "step is 245 and learning rate is 0.039 \n",
      "step is 246 and learning rate is 0.039 \n",
      "step is 247 and learning rate is 0.039 \n",
      "step is 248 and learning rate is 0.039 \n",
      "step is 249 and learning rate is 0.039 \n",
      "step is 250 and learning rate is 0.038 \n",
      "step is 251 and learning rate is 0.038 \n",
      "step is 252 and learning rate is 0.038 \n",
      "step is 253 and learning rate is 0.038 \n",
      "step is 254 and learning rate is 0.038 \n",
      "step is 255 and learning rate is 0.038 \n",
      "step is 256 and learning rate is 0.038 \n",
      "step is 257 and learning rate is 0.037 \n",
      "step is 258 and learning rate is 0.037 \n",
      "step is 259 and learning rate is 0.037 \n",
      "step is 260 and learning rate is 0.037 \n",
      "step is 261 and learning rate is 0.037 \n",
      "step is 262 and learning rate is 0.037 \n",
      "step is 263 and learning rate is 0.037 \n",
      "step is 264 and learning rate is 0.036 \n",
      "step is 265 and learning rate is 0.036 \n",
      "step is 266 and learning rate is 0.036 \n",
      "step is 267 and learning rate is 0.036 \n",
      "step is 268 and learning rate is 0.036 \n",
      "step is 269 and learning rate is 0.036 \n",
      "step is 270 and learning rate is 0.036 \n",
      "step is 271 and learning rate is 0.036 \n",
      "step is 272 and learning rate is 0.035 \n",
      "step is 273 and learning rate is 0.035 \n",
      "step is 274 and learning rate is 0.035 \n",
      "step is 275 and learning rate is 0.035 \n",
      "step is 276 and learning rate is 0.035 \n",
      "step is 277 and learning rate is 0.035 \n",
      "step is 278 and learning rate is 0.035 \n",
      "step is 279 and learning rate is 0.035 \n",
      "step is 280 and learning rate is 0.034 \n",
      "step is 281 and learning rate is 0.034 \n",
      "step is 282 and learning rate is 0.034 \n",
      "step is 283 and learning rate is 0.034 \n",
      "step is 284 and learning rate is 0.034 \n",
      "step is 285 and learning rate is 0.034 \n",
      "step is 286 and learning rate is 0.034 \n",
      "step is 287 and learning rate is 0.034 \n",
      "step is 288 and learning rate is 0.034 \n",
      "step is 289 and learning rate is 0.033 \n",
      "step is 290 and learning rate is 0.033 \n",
      "step is 291 and learning rate is 0.033 \n",
      "step is 292 and learning rate is 0.033 \n",
      "step is 293 and learning rate is 0.033 \n",
      "step is 294 and learning rate is 0.033 \n",
      "step is 295 and learning rate is 0.033 \n",
      "step is 296 and learning rate is 0.033 \n",
      "step is 297 and learning rate is 0.033 \n",
      "step is 298 and learning rate is 0.032 \n",
      "step is 299 and learning rate is 0.032 \n",
      "step is 300 and learning rate is 0.032 \n",
      "step is 301 and learning rate is 0.032 \n",
      "step is 302 and learning rate is 0.032 \n",
      "step is 303 and learning rate is 0.032 \n",
      "step is 304 and learning rate is 0.032 \n",
      "step is 305 and learning rate is 0.032 \n",
      "step is 306 and learning rate is 0.032 \n",
      "step is 307 and learning rate is 0.032 \n",
      "step is 308 and learning rate is 0.031 \n",
      "step is 309 and learning rate is 0.031 \n",
      "step is 310 and learning rate is 0.031 \n",
      "step is 311 and learning rate is 0.031 \n",
      "step is 312 and learning rate is 0.031 \n",
      "step is 313 and learning rate is 0.031 \n",
      "step is 314 and learning rate is 0.031 \n",
      "step is 315 and learning rate is 0.031 \n",
      "step is 316 and learning rate is 0.031 \n",
      "step is 317 and learning rate is 0.031 \n",
      "step is 318 and learning rate is 0.030 \n",
      "step is 319 and learning rate is 0.030 \n",
      "step is 320 and learning rate is 0.030 \n",
      "step is 321 and learning rate is 0.030 \n",
      "step is 322 and learning rate is 0.030 \n",
      "step is 323 and learning rate is 0.030 \n",
      "step is 324 and learning rate is 0.030 \n",
      "step is 325 and learning rate is 0.030 \n",
      "step is 326 and learning rate is 0.030 \n",
      "step is 327 and learning rate is 0.030 \n",
      "step is 328 and learning rate is 0.030 \n",
      "step is 329 and learning rate is 0.029 \n",
      "step is 330 and learning rate is 0.029 \n",
      "step is 331 and learning rate is 0.029 \n",
      "step is 332 and learning rate is 0.029 \n",
      "step is 333 and learning rate is 0.029 \n",
      "step is 334 and learning rate is 0.029 \n",
      "step is 335 and learning rate is 0.029 \n",
      "step is 336 and learning rate is 0.029 \n",
      "step is 337 and learning rate is 0.029 \n",
      "step is 338 and learning rate is 0.029 \n",
      "step is 339 and learning rate is 0.029 \n",
      "step is 340 and learning rate is 0.029 \n",
      "step is 341 and learning rate is 0.028 \n",
      "step is 342 and learning rate is 0.028 \n",
      "step is 343 and learning rate is 0.028 \n",
      "step is 344 and learning rate is 0.028 \n",
      "step is 345 and learning rate is 0.028 \n",
      "step is 346 and learning rate is 0.028 \n",
      "step is 347 and learning rate is 0.028 \n",
      "step is 348 and learning rate is 0.028 \n",
      "step is 349 and learning rate is 0.028 \n",
      "step is 350 and learning rate is 0.028 \n",
      "step is 351 and learning rate is 0.028 \n",
      "step is 352 and learning rate is 0.028 \n",
      "step is 353 and learning rate is 0.028 \n",
      "step is 354 and learning rate is 0.027 \n",
      "step is 355 and learning rate is 0.027 \n",
      "step is 356 and learning rate is 0.027 \n",
      "step is 357 and learning rate is 0.027 \n",
      "step is 358 and learning rate is 0.027 \n",
      "step is 359 and learning rate is 0.027 \n",
      "step is 360 and learning rate is 0.027 \n",
      "step is 361 and learning rate is 0.027 \n",
      "step is 362 and learning rate is 0.027 \n",
      "step is 363 and learning rate is 0.027 \n",
      "step is 364 and learning rate is 0.027 \n",
      "step is 365 and learning rate is 0.027 \n",
      "step is 366 and learning rate is 0.027 \n",
      "step is 367 and learning rate is 0.027 \n",
      "step is 368 and learning rate is 0.026 \n",
      "step is 369 and learning rate is 0.026 \n",
      "step is 370 and learning rate is 0.026 \n",
      "step is 371 and learning rate is 0.026 \n",
      "step is 372 and learning rate is 0.026 \n",
      "step is 373 and learning rate is 0.026 \n",
      "step is 374 and learning rate is 0.026 \n",
      "step is 375 and learning rate is 0.026 \n",
      "step is 376 and learning rate is 0.026 \n",
      "step is 377 and learning rate is 0.026 \n",
      "step is 378 and learning rate is 0.026 \n",
      "step is 379 and learning rate is 0.026 \n",
      "step is 380 and learning rate is 0.026 \n",
      "step is 381 and learning rate is 0.026 \n",
      "step is 382 and learning rate is 0.026 \n",
      "step is 383 and learning rate is 0.025 \n",
      "step is 384 and learning rate is 0.025 \n",
      "step is 385 and learning rate is 0.025 \n",
      "step is 386 and learning rate is 0.025 \n",
      "step is 387 and learning rate is 0.025 \n",
      "step is 388 and learning rate is 0.025 \n",
      "step is 389 and learning rate is 0.025 \n",
      "step is 390 and learning rate is 0.025 \n",
      "step is 391 and learning rate is 0.025 \n",
      "step is 392 and learning rate is 0.025 \n",
      "step is 393 and learning rate is 0.025 \n",
      "step is 394 and learning rate is 0.025 \n",
      "step is 395 and learning rate is 0.025 \n",
      "step is 396 and learning rate is 0.025 \n",
      "step is 397 and learning rate is 0.025 \n",
      "step is 398 and learning rate is 0.025 \n",
      "step is 399 and learning rate is 0.024 \n",
      "step is 400 and learning rate is 0.024 \n",
      "step is 401 and learning rate is 0.024 \n",
      "step is 402 and learning rate is 0.024 \n",
      "step is 403 and learning rate is 0.024 \n",
      "step is 404 and learning rate is 0.024 \n",
      "step is 405 and learning rate is 0.024 \n",
      "step is 406 and learning rate is 0.024 \n",
      "step is 407 and learning rate is 0.024 \n",
      "step is 408 and learning rate is 0.024 \n",
      "step is 409 and learning rate is 0.024 \n",
      "step is 410 and learning rate is 0.024 \n",
      "step is 411 and learning rate is 0.024 \n",
      "step is 412 and learning rate is 0.024 \n",
      "step is 413 and learning rate is 0.024 \n",
      "step is 414 and learning rate is 0.024 \n",
      "step is 415 and learning rate is 0.024 \n",
      "step is 416 and learning rate is 0.023 \n",
      "step is 417 and learning rate is 0.023 \n",
      "step is 418 and learning rate is 0.023 \n",
      "step is 419 and learning rate is 0.023 \n",
      "step is 420 and learning rate is 0.023 \n",
      "step is 421 and learning rate is 0.023 \n",
      "step is 422 and learning rate is 0.023 \n",
      "step is 423 and learning rate is 0.023 \n",
      "step is 424 and learning rate is 0.023 \n",
      "step is 425 and learning rate is 0.023 \n",
      "step is 426 and learning rate is 0.023 \n",
      "step is 427 and learning rate is 0.023 \n",
      "step is 428 and learning rate is 0.023 \n",
      "step is 429 and learning rate is 0.023 \n",
      "step is 430 and learning rate is 0.023 \n",
      "step is 431 and learning rate is 0.023 \n",
      "step is 432 and learning rate is 0.023 \n",
      "step is 433 and learning rate is 0.023 \n",
      "step is 434 and learning rate is 0.023 \n",
      "step is 435 and learning rate is 0.022 \n",
      "step is 436 and learning rate is 0.022 \n",
      "step is 437 and learning rate is 0.022 \n",
      "step is 438 and learning rate is 0.022 \n",
      "step is 439 and learning rate is 0.022 \n",
      "step is 440 and learning rate is 0.022 \n",
      "step is 441 and learning rate is 0.022 \n",
      "step is 442 and learning rate is 0.022 \n",
      "step is 443 and learning rate is 0.022 \n",
      "step is 444 and learning rate is 0.022 \n",
      "step is 445 and learning rate is 0.022 \n",
      "step is 446 and learning rate is 0.022 \n",
      "step is 447 and learning rate is 0.022 \n",
      "step is 448 and learning rate is 0.022 \n",
      "step is 449 and learning rate is 0.022 \n",
      "step is 450 and learning rate is 0.022 \n",
      "step is 451 and learning rate is 0.022 \n",
      "step is 452 and learning rate is 0.022 \n",
      "step is 453 and learning rate is 0.022 \n",
      "step is 454 and learning rate is 0.022 \n",
      "step is 455 and learning rate is 0.022 \n",
      "step is 456 and learning rate is 0.021 \n",
      "step is 457 and learning rate is 0.021 \n",
      "step is 458 and learning rate is 0.021 \n",
      "step is 459 and learning rate is 0.021 \n",
      "step is 460 and learning rate is 0.021 \n",
      "step is 461 and learning rate is 0.021 \n",
      "step is 462 and learning rate is 0.021 \n",
      "step is 463 and learning rate is 0.021 \n",
      "step is 464 and learning rate is 0.021 \n",
      "step is 465 and learning rate is 0.021 \n",
      "step is 466 and learning rate is 0.021 \n",
      "step is 467 and learning rate is 0.021 \n",
      "step is 468 and learning rate is 0.021 \n",
      "step is 469 and learning rate is 0.021 \n",
      "step is 470 and learning rate is 0.021 \n",
      "step is 471 and learning rate is 0.021 \n",
      "step is 472 and learning rate is 0.021 \n",
      "step is 473 and learning rate is 0.021 \n",
      "step is 474 and learning rate is 0.021 \n",
      "step is 475 and learning rate is 0.021 \n",
      "step is 476 and learning rate is 0.021 \n",
      "step is 477 and learning rate is 0.021 \n",
      "step is 478 and learning rate is 0.020 \n",
      "step is 479 and learning rate is 0.020 \n",
      "step is 480 and learning rate is 0.020 \n",
      "step is 481 and learning rate is 0.020 \n",
      "step is 482 and learning rate is 0.020 \n",
      "step is 483 and learning rate is 0.020 \n",
      "step is 484 and learning rate is 0.020 \n",
      "step is 485 and learning rate is 0.020 \n",
      "step is 486 and learning rate is 0.020 \n",
      "step is 487 and learning rate is 0.020 \n",
      "step is 488 and learning rate is 0.020 \n",
      "step is 489 and learning rate is 0.020 \n",
      "step is 490 and learning rate is 0.020 \n",
      "step is 491 and learning rate is 0.020 \n",
      "step is 492 and learning rate is 0.020 \n",
      "step is 493 and learning rate is 0.020 \n",
      "step is 494 and learning rate is 0.020 \n",
      "step is 495 and learning rate is 0.020 \n",
      "step is 496 and learning rate is 0.020 \n",
      "step is 497 and learning rate is 0.020 \n",
      "step is 498 and learning rate is 0.020 \n",
      "step is 499 and learning rate is 0.020 \n",
      "step is 500 and learning rate is 0.020 \n",
      "step is 501 and learning rate is 0.020 \n",
      "step is 502 and learning rate is 0.020 \n",
      "step is 503 and learning rate is 0.019 \n",
      "step is 504 and learning rate is 0.019 \n",
      "step is 505 and learning rate is 0.019 \n",
      "step is 506 and learning rate is 0.019 \n",
      "step is 507 and learning rate is 0.019 \n",
      "step is 508 and learning rate is 0.019 \n",
      "step is 509 and learning rate is 0.019 \n",
      "step is 510 and learning rate is 0.019 \n",
      "step is 511 and learning rate is 0.019 \n",
      "step is 512 and learning rate is 0.019 \n",
      "step is 513 and learning rate is 0.019 \n",
      "step is 514 and learning rate is 0.019 \n",
      "step is 515 and learning rate is 0.019 \n",
      "step is 516 and learning rate is 0.019 \n",
      "step is 517 and learning rate is 0.019 \n",
      "step is 518 and learning rate is 0.019 \n",
      "step is 519 and learning rate is 0.019 \n",
      "step is 520 and learning rate is 0.019 \n",
      "step is 521 and learning rate is 0.019 \n",
      "step is 522 and learning rate is 0.019 \n",
      "step is 523 and learning rate is 0.019 \n",
      "step is 524 and learning rate is 0.019 \n",
      "step is 525 and learning rate is 0.019 \n",
      "step is 526 and learning rate is 0.019 \n",
      "step is 527 and learning rate is 0.019 \n",
      "step is 528 and learning rate is 0.019 \n",
      "step is 529 and learning rate is 0.019 \n",
      "step is 530 and learning rate is 0.019 \n",
      "step is 531 and learning rate is 0.018 \n",
      "step is 532 and learning rate is 0.018 \n",
      "step is 533 and learning rate is 0.018 \n",
      "step is 534 and learning rate is 0.018 \n",
      "step is 535 and learning rate is 0.018 \n",
      "step is 536 and learning rate is 0.018 \n",
      "step is 537 and learning rate is 0.018 \n",
      "step is 538 and learning rate is 0.018 \n",
      "step is 539 and learning rate is 0.018 \n",
      "step is 540 and learning rate is 0.018 \n",
      "step is 541 and learning rate is 0.018 \n",
      "step is 542 and learning rate is 0.018 \n",
      "step is 543 and learning rate is 0.018 \n",
      "step is 544 and learning rate is 0.018 \n",
      "step is 545 and learning rate is 0.018 \n",
      "step is 546 and learning rate is 0.018 \n",
      "step is 547 and learning rate is 0.018 \n",
      "step is 548 and learning rate is 0.018 \n",
      "step is 549 and learning rate is 0.018 \n",
      "step is 550 and learning rate is 0.018 \n",
      "step is 551 and learning rate is 0.018 \n",
      "step is 552 and learning rate is 0.018 \n",
      "step is 553 and learning rate is 0.018 \n",
      "step is 554 and learning rate is 0.018 \n",
      "step is 555 and learning rate is 0.018 \n",
      "step is 556 and learning rate is 0.018 \n",
      "step is 557 and learning rate is 0.018 \n",
      "step is 558 and learning rate is 0.018 \n",
      "step is 559 and learning rate is 0.018 \n",
      "step is 560 and learning rate is 0.018 \n",
      "step is 561 and learning rate is 0.018 \n",
      "step is 562 and learning rate is 0.017 \n",
      "step is 563 and learning rate is 0.017 \n",
      "step is 564 and learning rate is 0.017 \n",
      "step is 565 and learning rate is 0.017 \n",
      "step is 566 and learning rate is 0.017 \n",
      "step is 567 and learning rate is 0.017 \n",
      "step is 568 and learning rate is 0.017 \n",
      "step is 569 and learning rate is 0.017 \n",
      "step is 570 and learning rate is 0.017 \n",
      "step is 571 and learning rate is 0.017 \n",
      "step is 572 and learning rate is 0.017 \n",
      "step is 573 and learning rate is 0.017 \n",
      "step is 574 and learning rate is 0.017 \n",
      "step is 575 and learning rate is 0.017 \n",
      "step is 576 and learning rate is 0.017 \n",
      "step is 577 and learning rate is 0.017 \n",
      "step is 578 and learning rate is 0.017 \n",
      "step is 579 and learning rate is 0.017 \n",
      "step is 580 and learning rate is 0.017 \n",
      "step is 581 and learning rate is 0.017 \n",
      "step is 582 and learning rate is 0.017 \n",
      "step is 583 and learning rate is 0.017 \n",
      "step is 584 and learning rate is 0.017 \n",
      "step is 585 and learning rate is 0.017 \n",
      "step is 586 and learning rate is 0.017 \n",
      "step is 587 and learning rate is 0.017 \n",
      "step is 588 and learning rate is 0.017 \n",
      "step is 589 and learning rate is 0.017 \n",
      "step is 590 and learning rate is 0.017 \n",
      "step is 591 and learning rate is 0.017 \n",
      "step is 592 and learning rate is 0.017 \n",
      "step is 593 and learning rate is 0.017 \n",
      "step is 594 and learning rate is 0.017 \n",
      "step is 595 and learning rate is 0.017 \n",
      "step is 596 and learning rate is 0.017 \n",
      "step is 597 and learning rate is 0.016 \n",
      "step is 598 and learning rate is 0.016 \n",
      "step is 599 and learning rate is 0.016 \n",
      "step is 600 and learning rate is 0.016 \n",
      "step is 601 and learning rate is 0.016 \n",
      "step is 602 and learning rate is 0.016 \n",
      "step is 603 and learning rate is 0.016 \n",
      "step is 604 and learning rate is 0.016 \n",
      "step is 605 and learning rate is 0.016 \n",
      "step is 606 and learning rate is 0.016 \n",
      "step is 607 and learning rate is 0.016 \n",
      "step is 608 and learning rate is 0.016 \n",
      "step is 609 and learning rate is 0.016 \n",
      "step is 610 and learning rate is 0.016 \n",
      "step is 611 and learning rate is 0.016 \n",
      "step is 612 and learning rate is 0.016 \n",
      "step is 613 and learning rate is 0.016 \n",
      "step is 614 and learning rate is 0.016 \n",
      "step is 615 and learning rate is 0.016 \n",
      "step is 616 and learning rate is 0.016 \n",
      "step is 617 and learning rate is 0.016 \n",
      "step is 618 and learning rate is 0.016 \n",
      "step is 619 and learning rate is 0.016 \n",
      "step is 620 and learning rate is 0.016 \n",
      "step is 621 and learning rate is 0.016 \n",
      "step is 622 and learning rate is 0.016 \n",
      "step is 623 and learning rate is 0.016 \n",
      "step is 624 and learning rate is 0.016 \n",
      "step is 625 and learning rate is 0.016 \n",
      "step is 626 and learning rate is 0.016 \n",
      "step is 627 and learning rate is 0.016 \n",
      "step is 628 and learning rate is 0.016 \n",
      "step is 629 and learning rate is 0.016 \n",
      "step is 630 and learning rate is 0.016 \n",
      "step is 631 and learning rate is 0.016 \n",
      "step is 632 and learning rate is 0.016 \n",
      "step is 633 and learning rate is 0.016 \n",
      "step is 634 and learning rate is 0.016 \n",
      "step is 635 and learning rate is 0.016 \n",
      "step is 636 and learning rate is 0.015 \n",
      "step is 637 and learning rate is 0.015 \n",
      "step is 638 and learning rate is 0.015 \n",
      "step is 639 and learning rate is 0.015 \n",
      "step is 640 and learning rate is 0.015 \n",
      "step is 641 and learning rate is 0.015 \n",
      "step is 642 and learning rate is 0.015 \n",
      "step is 643 and learning rate is 0.015 \n",
      "step is 644 and learning rate is 0.015 \n",
      "step is 645 and learning rate is 0.015 \n",
      "step is 646 and learning rate is 0.015 \n",
      "step is 647 and learning rate is 0.015 \n",
      "step is 648 and learning rate is 0.015 \n",
      "step is 649 and learning rate is 0.015 \n",
      "step is 650 and learning rate is 0.015 \n",
      "step is 651 and learning rate is 0.015 \n",
      "step is 652 and learning rate is 0.015 \n",
      "step is 653 and learning rate is 0.015 \n",
      "step is 654 and learning rate is 0.015 \n",
      "step is 655 and learning rate is 0.015 \n",
      "step is 656 and learning rate is 0.015 \n",
      "step is 657 and learning rate is 0.015 \n",
      "step is 658 and learning rate is 0.015 \n",
      "step is 659 and learning rate is 0.015 \n",
      "step is 660 and learning rate is 0.015 \n",
      "step is 661 and learning rate is 0.015 \n",
      "step is 662 and learning rate is 0.015 \n",
      "step is 663 and learning rate is 0.015 \n",
      "step is 664 and learning rate is 0.015 \n",
      "step is 665 and learning rate is 0.015 \n",
      "step is 666 and learning rate is 0.015 \n",
      "step is 667 and learning rate is 0.015 \n",
      "step is 668 and learning rate is 0.015 \n",
      "step is 669 and learning rate is 0.015 \n",
      "step is 670 and learning rate is 0.015 \n",
      "step is 671 and learning rate is 0.015 \n",
      "step is 672 and learning rate is 0.015 \n",
      "step is 673 and learning rate is 0.015 \n",
      "step is 674 and learning rate is 0.015 \n",
      "step is 675 and learning rate is 0.015 \n",
      "step is 676 and learning rate is 0.015 \n",
      "step is 677 and learning rate is 0.015 \n",
      "step is 678 and learning rate is 0.015 \n",
      "step is 679 and learning rate is 0.015 \n",
      "step is 680 and learning rate is 0.014 \n",
      "step is 681 and learning rate is 0.014 \n",
      "step is 682 and learning rate is 0.014 \n",
      "step is 683 and learning rate is 0.014 \n",
      "step is 684 and learning rate is 0.014 \n",
      "step is 685 and learning rate is 0.014 \n",
      "step is 686 and learning rate is 0.014 \n",
      "step is 687 and learning rate is 0.014 \n",
      "step is 688 and learning rate is 0.014 \n",
      "step is 689 and learning rate is 0.014 \n",
      "step is 690 and learning rate is 0.014 \n",
      "step is 691 and learning rate is 0.014 \n",
      "step is 692 and learning rate is 0.014 \n",
      "step is 693 and learning rate is 0.014 \n",
      "step is 694 and learning rate is 0.014 \n",
      "step is 695 and learning rate is 0.014 \n",
      "step is 696 and learning rate is 0.014 \n",
      "step is 697 and learning rate is 0.014 \n",
      "step is 698 and learning rate is 0.014 \n",
      "step is 699 and learning rate is 0.014 \n",
      "step is 700 and learning rate is 0.014 \n",
      "step is 701 and learning rate is 0.014 \n",
      "step is 702 and learning rate is 0.014 \n",
      "step is 703 and learning rate is 0.014 \n",
      "step is 704 and learning rate is 0.014 \n",
      "step is 705 and learning rate is 0.014 \n",
      "step is 706 and learning rate is 0.014 \n",
      "step is 707 and learning rate is 0.014 \n",
      "step is 708 and learning rate is 0.014 \n",
      "step is 709 and learning rate is 0.014 \n",
      "step is 710 and learning rate is 0.014 \n",
      "step is 711 and learning rate is 0.014 \n",
      "step is 712 and learning rate is 0.014 \n",
      "step is 713 and learning rate is 0.014 \n",
      "step is 714 and learning rate is 0.014 \n",
      "step is 715 and learning rate is 0.014 \n",
      "step is 716 and learning rate is 0.014 \n",
      "step is 717 and learning rate is 0.014 \n",
      "step is 718 and learning rate is 0.014 \n",
      "step is 719 and learning rate is 0.014 \n",
      "step is 720 and learning rate is 0.014 \n",
      "step is 721 and learning rate is 0.014 \n",
      "step is 722 and learning rate is 0.014 \n",
      "step is 723 and learning rate is 0.014 \n",
      "step is 724 and learning rate is 0.014 \n",
      "step is 725 and learning rate is 0.014 \n",
      "step is 726 and learning rate is 0.014 \n",
      "step is 727 and learning rate is 0.014 \n",
      "step is 728 and learning rate is 0.014 \n",
      "step is 729 and learning rate is 0.014 \n",
      "step is 730 and learning rate is 0.014 \n",
      "step is 731 and learning rate is 0.013 \n",
      "step is 732 and learning rate is 0.013 \n",
      "step is 733 and learning rate is 0.013 \n",
      "step is 734 and learning rate is 0.013 \n",
      "step is 735 and learning rate is 0.013 \n",
      "step is 736 and learning rate is 0.013 \n",
      "step is 737 and learning rate is 0.013 \n",
      "step is 738 and learning rate is 0.013 \n",
      "step is 739 and learning rate is 0.013 \n",
      "step is 740 and learning rate is 0.013 \n",
      "step is 741 and learning rate is 0.013 \n",
      "step is 742 and learning rate is 0.013 \n",
      "step is 743 and learning rate is 0.013 \n",
      "step is 744 and learning rate is 0.013 \n",
      "step is 745 and learning rate is 0.013 \n",
      "step is 746 and learning rate is 0.013 \n",
      "step is 747 and learning rate is 0.013 \n",
      "step is 748 and learning rate is 0.013 \n",
      "step is 749 and learning rate is 0.013 \n",
      "step is 750 and learning rate is 0.013 \n",
      "step is 751 and learning rate is 0.013 \n",
      "step is 752 and learning rate is 0.013 \n",
      "step is 753 and learning rate is 0.013 \n",
      "step is 754 and learning rate is 0.013 \n",
      "step is 755 and learning rate is 0.013 \n",
      "step is 756 and learning rate is 0.013 \n",
      "step is 757 and learning rate is 0.013 \n",
      "step is 758 and learning rate is 0.013 \n",
      "step is 759 and learning rate is 0.013 \n",
      "step is 760 and learning rate is 0.013 \n",
      "step is 761 and learning rate is 0.013 \n",
      "step is 762 and learning rate is 0.013 \n",
      "step is 763 and learning rate is 0.013 \n",
      "step is 764 and learning rate is 0.013 \n",
      "step is 765 and learning rate is 0.013 \n",
      "step is 766 and learning rate is 0.013 \n",
      "step is 767 and learning rate is 0.013 \n",
      "step is 768 and learning rate is 0.013 \n",
      "step is 769 and learning rate is 0.013 \n",
      "step is 770 and learning rate is 0.013 \n",
      "step is 771 and learning rate is 0.013 \n",
      "step is 772 and learning rate is 0.013 \n",
      "step is 773 and learning rate is 0.013 \n",
      "step is 774 and learning rate is 0.013 \n",
      "step is 775 and learning rate is 0.013 \n",
      "step is 776 and learning rate is 0.013 \n",
      "step is 777 and learning rate is 0.013 \n",
      "step is 778 and learning rate is 0.013 \n",
      "step is 779 and learning rate is 0.013 \n",
      "step is 780 and learning rate is 0.013 \n",
      "step is 781 and learning rate is 0.013 \n",
      "step is 782 and learning rate is 0.013 \n",
      "step is 783 and learning rate is 0.013 \n",
      "step is 784 and learning rate is 0.013 \n",
      "step is 785 and learning rate is 0.013 \n",
      "step is 786 and learning rate is 0.013 \n",
      "step is 787 and learning rate is 0.013 \n",
      "step is 788 and learning rate is 0.013 \n",
      "step is 789 and learning rate is 0.013 \n",
      "step is 790 and learning rate is 0.013 \n",
      "step is 791 and learning rate is 0.012 \n",
      "step is 792 and learning rate is 0.012 \n",
      "step is 793 and learning rate is 0.012 \n",
      "step is 794 and learning rate is 0.012 \n",
      "step is 795 and learning rate is 0.012 \n",
      "step is 796 and learning rate is 0.012 \n",
      "step is 797 and learning rate is 0.012 \n",
      "step is 798 and learning rate is 0.012 \n",
      "step is 799 and learning rate is 0.012 \n",
      "step is 800 and learning rate is 0.012 \n",
      "step is 801 and learning rate is 0.012 \n",
      "step is 802 and learning rate is 0.012 \n",
      "step is 803 and learning rate is 0.012 \n",
      "step is 804 and learning rate is 0.012 \n",
      "step is 805 and learning rate is 0.012 \n",
      "step is 806 and learning rate is 0.012 \n",
      "step is 807 and learning rate is 0.012 \n",
      "step is 808 and learning rate is 0.012 \n",
      "step is 809 and learning rate is 0.012 \n",
      "step is 810 and learning rate is 0.012 \n",
      "step is 811 and learning rate is 0.012 \n",
      "step is 812 and learning rate is 0.012 \n",
      "step is 813 and learning rate is 0.012 \n",
      "step is 814 and learning rate is 0.012 \n",
      "step is 815 and learning rate is 0.012 \n",
      "step is 816 and learning rate is 0.012 \n",
      "step is 817 and learning rate is 0.012 \n",
      "step is 818 and learning rate is 0.012 \n",
      "step is 819 and learning rate is 0.012 \n",
      "step is 820 and learning rate is 0.012 \n",
      "step is 821 and learning rate is 0.012 \n",
      "step is 822 and learning rate is 0.012 \n",
      "step is 823 and learning rate is 0.012 \n",
      "step is 824 and learning rate is 0.012 \n",
      "step is 825 and learning rate is 0.012 \n",
      "step is 826 and learning rate is 0.012 \n",
      "step is 827 and learning rate is 0.012 \n",
      "step is 828 and learning rate is 0.012 \n",
      "step is 829 and learning rate is 0.012 \n",
      "step is 830 and learning rate is 0.012 \n",
      "step is 831 and learning rate is 0.012 \n",
      "step is 832 and learning rate is 0.012 \n",
      "step is 833 and learning rate is 0.012 \n",
      "step is 834 and learning rate is 0.012 \n",
      "step is 835 and learning rate is 0.012 \n",
      "step is 836 and learning rate is 0.012 \n",
      "step is 837 and learning rate is 0.012 \n",
      "step is 838 and learning rate is 0.012 \n",
      "step is 839 and learning rate is 0.012 \n",
      "step is 840 and learning rate is 0.012 \n",
      "step is 841 and learning rate is 0.012 \n",
      "step is 842 and learning rate is 0.012 \n",
      "step is 843 and learning rate is 0.012 \n",
      "step is 844 and learning rate is 0.012 \n",
      "step is 845 and learning rate is 0.012 \n",
      "step is 846 and learning rate is 0.012 \n",
      "step is 847 and learning rate is 0.012 \n",
      "step is 848 and learning rate is 0.012 \n",
      "step is 849 and learning rate is 0.012 \n",
      "step is 850 and learning rate is 0.012 \n",
      "step is 851 and learning rate is 0.012 \n",
      "step is 852 and learning rate is 0.012 \n",
      "step is 853 and learning rate is 0.012 \n",
      "step is 854 and learning rate is 0.012 \n",
      "step is 855 and learning rate is 0.012 \n",
      "step is 856 and learning rate is 0.012 \n",
      "step is 857 and learning rate is 0.012 \n",
      "step is 858 and learning rate is 0.012 \n",
      "step is 859 and learning rate is 0.012 \n",
      "step is 860 and learning rate is 0.011 \n",
      "step is 861 and learning rate is 0.011 \n",
      "step is 862 and learning rate is 0.011 \n",
      "step is 863 and learning rate is 0.011 \n",
      "step is 864 and learning rate is 0.011 \n",
      "step is 865 and learning rate is 0.011 \n",
      "step is 866 and learning rate is 0.011 \n",
      "step is 867 and learning rate is 0.011 \n",
      "step is 868 and learning rate is 0.011 \n",
      "step is 869 and learning rate is 0.011 \n",
      "step is 870 and learning rate is 0.011 \n",
      "step is 871 and learning rate is 0.011 \n",
      "step is 872 and learning rate is 0.011 \n",
      "step is 873 and learning rate is 0.011 \n",
      "step is 874 and learning rate is 0.011 \n",
      "step is 875 and learning rate is 0.011 \n",
      "step is 876 and learning rate is 0.011 \n",
      "step is 877 and learning rate is 0.011 \n",
      "step is 878 and learning rate is 0.011 \n",
      "step is 879 and learning rate is 0.011 \n",
      "step is 880 and learning rate is 0.011 \n",
      "step is 881 and learning rate is 0.011 \n",
      "step is 882 and learning rate is 0.011 \n",
      "step is 883 and learning rate is 0.011 \n",
      "step is 884 and learning rate is 0.011 \n",
      "step is 885 and learning rate is 0.011 \n",
      "step is 886 and learning rate is 0.011 \n",
      "step is 887 and learning rate is 0.011 \n",
      "step is 888 and learning rate is 0.011 \n",
      "step is 889 and learning rate is 0.011 \n",
      "step is 890 and learning rate is 0.011 \n",
      "step is 891 and learning rate is 0.011 \n",
      "step is 892 and learning rate is 0.011 \n",
      "step is 893 and learning rate is 0.011 \n",
      "step is 894 and learning rate is 0.011 \n",
      "step is 895 and learning rate is 0.011 \n",
      "step is 896 and learning rate is 0.011 \n",
      "step is 897 and learning rate is 0.011 \n",
      "step is 898 and learning rate is 0.011 \n",
      "step is 899 and learning rate is 0.011 \n",
      "step is 900 and learning rate is 0.011 \n",
      "step is 901 and learning rate is 0.011 \n",
      "step is 902 and learning rate is 0.011 \n",
      "step is 903 and learning rate is 0.011 \n",
      "step is 904 and learning rate is 0.011 \n",
      "step is 905 and learning rate is 0.011 \n",
      "step is 906 and learning rate is 0.011 \n",
      "step is 907 and learning rate is 0.011 \n",
      "step is 908 and learning rate is 0.011 \n",
      "step is 909 and learning rate is 0.011 \n",
      "step is 910 and learning rate is 0.011 \n",
      "step is 911 and learning rate is 0.011 \n",
      "step is 912 and learning rate is 0.011 \n",
      "step is 913 and learning rate is 0.011 \n",
      "step is 914 and learning rate is 0.011 \n",
      "step is 915 and learning rate is 0.011 \n",
      "step is 916 and learning rate is 0.011 \n",
      "step is 917 and learning rate is 0.011 \n",
      "step is 918 and learning rate is 0.011 \n",
      "step is 919 and learning rate is 0.011 \n",
      "step is 920 and learning rate is 0.011 \n",
      "step is 921 and learning rate is 0.011 \n",
      "step is 922 and learning rate is 0.011 \n",
      "step is 923 and learning rate is 0.011 \n",
      "step is 924 and learning rate is 0.011 \n",
      "step is 925 and learning rate is 0.011 \n",
      "step is 926 and learning rate is 0.011 \n",
      "step is 927 and learning rate is 0.011 \n",
      "step is 928 and learning rate is 0.011 \n",
      "step is 929 and learning rate is 0.011 \n",
      "step is 930 and learning rate is 0.011 \n",
      "step is 931 and learning rate is 0.011 \n",
      "step is 932 and learning rate is 0.011 \n",
      "step is 933 and learning rate is 0.011 \n",
      "step is 934 and learning rate is 0.011 \n",
      "step is 935 and learning rate is 0.011 \n",
      "step is 936 and learning rate is 0.011 \n",
      "step is 937 and learning rate is 0.011 \n",
      "step is 938 and learning rate is 0.011 \n",
      "step is 939 and learning rate is 0.011 \n",
      "step is 940 and learning rate is 0.011 \n",
      "step is 941 and learning rate is 0.011 \n",
      "step is 942 and learning rate is 0.011 \n",
      "step is 943 and learning rate is 0.010 \n",
      "step is 944 and learning rate is 0.010 \n",
      "step is 945 and learning rate is 0.010 \n",
      "step is 946 and learning rate is 0.010 \n",
      "step is 947 and learning rate is 0.010 \n",
      "step is 948 and learning rate is 0.010 \n",
      "step is 949 and learning rate is 0.010 \n",
      "step is 950 and learning rate is 0.010 \n",
      "step is 951 and learning rate is 0.010 \n",
      "step is 952 and learning rate is 0.010 \n",
      "step is 953 and learning rate is 0.010 \n",
      "step is 954 and learning rate is 0.010 \n",
      "step is 955 and learning rate is 0.010 \n",
      "step is 956 and learning rate is 0.010 \n",
      "step is 957 and learning rate is 0.010 \n",
      "step is 958 and learning rate is 0.010 \n",
      "step is 959 and learning rate is 0.010 \n",
      "step is 960 and learning rate is 0.010 \n",
      "step is 961 and learning rate is 0.010 \n",
      "step is 962 and learning rate is 0.010 \n",
      "step is 963 and learning rate is 0.010 \n",
      "step is 964 and learning rate is 0.010 \n",
      "step is 965 and learning rate is 0.010 \n",
      "step is 966 and learning rate is 0.010 \n",
      "step is 967 and learning rate is 0.010 \n",
      "step is 968 and learning rate is 0.010 \n",
      "step is 969 and learning rate is 0.010 \n",
      "step is 970 and learning rate is 0.010 \n",
      "step is 971 and learning rate is 0.010 \n",
      "step is 972 and learning rate is 0.010 \n",
      "step is 973 and learning rate is 0.010 \n",
      "step is 974 and learning rate is 0.010 \n",
      "step is 975 and learning rate is 0.010 \n",
      "step is 976 and learning rate is 0.010 \n",
      "step is 977 and learning rate is 0.010 \n",
      "step is 978 and learning rate is 0.010 \n",
      "step is 979 and learning rate is 0.010 \n",
      "step is 980 and learning rate is 0.010 \n",
      "step is 981 and learning rate is 0.010 \n",
      "step is 982 and learning rate is 0.010 \n",
      "step is 983 and learning rate is 0.010 \n",
      "step is 984 and learning rate is 0.010 \n",
      "step is 985 and learning rate is 0.010 \n",
      "step is 986 and learning rate is 0.010 \n",
      "step is 987 and learning rate is 0.010 \n",
      "step is 988 and learning rate is 0.010 \n",
      "step is 989 and learning rate is 0.010 \n",
      "step is 990 and learning rate is 0.010 \n",
      "step is 991 and learning rate is 0.010 \n",
      "step is 992 and learning rate is 0.010 \n",
      "step is 993 and learning rate is 0.010 \n",
      "step is 994 and learning rate is 0.010 \n",
      "step is 995 and learning rate is 0.010 \n",
      "step is 996 and learning rate is 0.010 \n",
      "step is 997 and learning rate is 0.010 \n",
      "step is 998 and learning rate is 0.010 \n",
      "step is 999 and learning rate is 0.010 \n"
     ]
    }
   ],
   "source": [
    "#applying learnign rate decay\n",
    "starting_learning_rate=1\n",
    "decay_rate=0.1\n",
    "\n",
    "\n",
    "for step in range(1000):\n",
    "    learning_rate=starting_learning_rate/(1+step*decay_rate)\n",
    "    print(f\"step is {step} and learning rate is {learning_rate:.3f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67eaaa-310a-4668-8778-d7a632eb6b65",
   "metadata": {},
   "source": [
    "# added learning rate update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8f85f4-b876-447f-890f-aefbfc1f1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets build a neural network model\n",
    "\n",
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = SGD(lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2452dde0-122b-4690-b89f-466934bbbaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | accuracy is 0.33 |  loss is 2.17 | learning rate is 0.01 \n",
      "epoch 1000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n",
      "epoch 2000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n",
      "epoch 3000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n",
      "epoch 4000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n",
      "epoch 5000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n",
      "epoch 6000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n",
      "epoch 7000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n",
      "epoch 8000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n",
      "epoch 9000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n",
      "epoch 10000 | accuracy is 0.33 |  loss is 10.75 | learning rate is 0.01 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    #lets see the forward pass of the model\n",
    "    layer1.forward(x)\n",
    "    \n",
    "    act1.forward(layer1.outputs)\n",
    "    \n",
    "    layer2.forward(act1.output)\n",
    "    \n",
    "    loss=act_loss.forward(layer2.outputs,y)\n",
    "    \n",
    "    #lets display the loss of the model\n",
    "    \n",
    "    #lets calculate the accuracy of the model\n",
    "    prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "    \n",
    "    if len(y.shape)==2:\n",
    "        y=argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(prediction==y)\n",
    "\n",
    "    #lets print some\n",
    "    \n",
    "    #the accuracy of the model is\n",
    "    if i%1000==0:\n",
    "        print(f\"epoch {i} | accuracy is {accuracy:.2f} |  loss is {loss:.2f} | learning rate is {optimizer.current_learning_rate} \")\n",
    "    \n",
    "    #lets backward pass\n",
    "    act_loss.backward(act_loss.outputs,y)\n",
    "    layer2.backward(act_loss.dinputs)\n",
    "    act1.backward(layer2.dinputs)\n",
    "    layer1.backward(act1.drelu)\n",
    "\n",
    "    #lets optimize the model\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2ec86-468e-4474-8753-5107c61d9175",
   "metadata": {},
   "source": [
    "## lets try decreasing our learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e309dcf8-1b67-47db-99f4-3bd76179a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets build a neural network model\n",
    "\n",
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = SGD(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea2af770-7b9a-49dc-b47d-478669d761a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | accuracy is 0.30 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 1000 | accuracy is 0.39 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 2000 | accuracy is 0.36 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 3000 | accuracy is 0.41 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 4000 | accuracy is 0.42 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 5000 | accuracy is 0.42 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 6000 | accuracy is 0.42 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 7000 | accuracy is 0.42 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 8000 | accuracy is 0.42 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 9000 | accuracy is 0.42 |  loss is 1.10 | learning rate is 0.001 \n",
      "epoch 10000 | accuracy is 0.42 |  loss is 1.10 | learning rate is 0.001 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    #lets see the forward pass of the model\n",
    "    layer1.forward(x)\n",
    "    \n",
    "    act1.forward(layer1.outputs)\n",
    "    \n",
    "    layer2.forward(act1.output)\n",
    "    \n",
    "    loss=act_loss.forward(layer2.outputs,y)\n",
    "    \n",
    "    #lets display the loss of the model\n",
    "    \n",
    "    #lets calculate the accuracy of the model\n",
    "    prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "    \n",
    "    if len(y.shape)==2:\n",
    "        y=argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(prediction==y)\n",
    "\n",
    "    #lets print some\n",
    "    \n",
    "    #the accuracy of the model is\n",
    "    if i%1000==0:\n",
    "        print(f\"epoch {i} | accuracy is {accuracy:.2f} |  loss is {loss:.2f} | learning rate is {optimizer.current_learning_rate} \")\n",
    "    \n",
    "    #lets backward pass\n",
    "    act_loss.backward(act_loss.outputs,y)\n",
    "    layer2.backward(act_loss.dinputs)\n",
    "    act1.backward(layer2.dinputs)\n",
    "    layer1.backward(act1.drelu)\n",
    "\n",
    "    #lets optimize the model\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4625f-0924-401e-9072-9c941724a9ac",
   "metadata": {},
   "source": [
    "# lets add momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84495a3d-aa99-4cf4-a98f-d6bd41cb8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets build a neural network model\n",
    "\n",
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = SGD(lr=1,decay_rate=1e-3,momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da856bfc-88dc-4281-9112-20e7aa9b7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10001):\n",
    "    #lets see the forward pass of the model\n",
    "    layer1.forward(x)\n",
    "    \n",
    "    act1.forward(layer1.outputs)\n",
    "    \n",
    "    layer2.forward(act1.output)\n",
    "    \n",
    "    loss=act_loss.forward(layer2.outputs,y)\n",
    "    \n",
    "    #lets display the loss of the model\n",
    "    \n",
    "    #lets calculate the accuracy of the model\n",
    "    prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "    \n",
    "    if len(y.shape)==2:\n",
    "        y=argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(prediction==y)\n",
    "\n",
    "    #lets print some\n",
    "    \n",
    "    #the accuracy of the model is\n",
    "    if i%1000==0:\n",
    "        print(f\"epoch {i} | accuracy is {accuracy:.2f} |  loss is {loss:.3f} | learning rate is {optimizer.current_learning_rate} \")\n",
    "    \n",
    "    #lets backward pass\n",
    "    act_loss.backward(act_loss.outputs,y)\n",
    "    layer2.backward(act_loss.dinputs)\n",
    "    act1.backward(layer2.dinputs)\n",
    "    layer1.backward(act1.drelu)\n",
    "\n",
    "    #lets optimize the model\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff9f0c-46f4-4f1a-888e-d5adce5da3cc",
   "metadata": {},
   "source": [
    "# lets train with momentum that is 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c737117-8b14-4d75-b502-460bd4696203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets build a neural network model\n",
    "\n",
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = SGD(lr=1,decay_rate=1e-3,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d102c76-7dfe-4432-a3d8-ce8f2eecdfbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | accuracy is 0.38 |  loss is 1.099 | learning rate is 1.000 \n",
      "epoch 1000 | accuracy is 0.42 |  loss is 1.056 | learning rate is 0.500 \n",
      "epoch 2000 | accuracy is 0.48 |  loss is 1.018 | learning rate is 0.333 \n",
      "epoch 3000 | accuracy is 0.50 |  loss is 1.003 | learning rate is 0.250 \n",
      "epoch 4000 | accuracy is 0.50 |  loss is 0.951 | learning rate is 0.200 \n",
      "epoch 5000 | accuracy is 0.61 |  loss is 0.830 | learning rate is 0.167 \n",
      "epoch 6000 | accuracy is 0.60 |  loss is 0.867 | learning rate is 0.143 \n",
      "epoch 7000 | accuracy is 0.64 |  loss is 0.798 | learning rate is 0.125 \n",
      "epoch 8000 | accuracy is 0.69 |  loss is 0.698 | learning rate is 0.111 \n",
      "epoch 9000 | accuracy is 0.68 |  loss is 0.764 | learning rate is 0.100 \n",
      "epoch 10000 | accuracy is 0.68 |  loss is 0.682 | learning rate is 0.091 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    #lets see the forward pass of the model\n",
    "    layer1.forward(x)\n",
    "    \n",
    "    act1.forward(layer1.outputs)\n",
    "    \n",
    "    layer2.forward(act1.output)\n",
    "    \n",
    "    loss=act_loss.forward(layer2.outputs,y)\n",
    "    \n",
    "    #lets display the loss of the model\n",
    "    \n",
    "    #lets calculate the accuracy of the model\n",
    "    prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "    \n",
    "    if len(y.shape)==2:\n",
    "        y=argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(prediction==y)\n",
    "\n",
    "    #lets print some\n",
    "    \n",
    "    #the accuracy of the model is\n",
    "    if i%1000==0:\n",
    "        print(f\"epoch {i} | accuracy is {accuracy:.2f} |  loss is {loss:.3f} | learning rate is {optimizer.current_learning_rate:.3f} \")\n",
    "    \n",
    "    #lets backward pass\n",
    "    act_loss.backward(act_loss.outputs,y)\n",
    "    layer2.backward(act_loss.dinputs)\n",
    "    act1.backward(layer2.dinputs)\n",
    "    layer1.backward(act1.drelu)\n",
    "\n",
    "    #lets optimize the model\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af3dfc-84ee-414a-b2fd-5784eb8f00f5",
   "metadata": {},
   "source": [
    "# lets use AdaGrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dfacb7c-de1a-4951-bd3b-d58e505fe71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = Ada_Grad(lr=1,decay_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73bb4446-40ee-48d4-b98d-9c5ccfe09c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | accuracy is 0.31 |  loss is 1.099 | learning rate is 1.00000 \n",
      "epoch 1000 | accuracy is 0.71 |  loss is 0.653 | learning rate is 0.00100 \n",
      "epoch 2000 | accuracy is 0.74 |  loss is 0.526 | learning rate is 0.00050 \n",
      "epoch 3000 | accuracy is 0.79 |  loss is 0.462 | learning rate is 0.00033 \n",
      "epoch 4000 | accuracy is 0.80 |  loss is 0.433 | learning rate is 0.00025 \n",
      "epoch 5000 | accuracy is 0.81 |  loss is 0.413 | learning rate is 0.00020 \n",
      "epoch 6000 | accuracy is 0.82 |  loss is 0.393 | learning rate is 0.00017 \n",
      "epoch 7000 | accuracy is 0.84 |  loss is 0.365 | learning rate is 0.00014 \n",
      "epoch 8000 | accuracy is 0.85 |  loss is 0.348 | learning rate is 0.00013 \n",
      "epoch 9000 | accuracy is 0.86 |  loss is 0.337 | learning rate is 0.00011 \n",
      "epoch 10000 | accuracy is 0.86 |  loss is 0.330 | learning rate is 0.00010 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    #lets see the forward pass of the model\n",
    "    layer1.forward(x)\n",
    "    \n",
    "    act1.forward(layer1.outputs)\n",
    "    \n",
    "    layer2.forward(act1.output)\n",
    "    \n",
    "    loss=act_loss.forward(layer2.outputs,y)\n",
    "    \n",
    "    #lets display the loss of the model\n",
    "    \n",
    "    #lets calculate the accuracy of the model\n",
    "    prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "    \n",
    "    if len(y.shape)==2:\n",
    "        y=argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(prediction==y)\n",
    "\n",
    "    #lets print some\n",
    "    \n",
    "    #the accuracy of the model is\n",
    "    if i%1000==0:\n",
    "        print(f\"epoch {i} | accuracy is {accuracy:.2f} |  loss is {loss:.3f} | learning rate is {optimizer.current_learning_rate:.5f} \")\n",
    "    \n",
    "    #lets backward pass\n",
    "    act_loss.backward(act_loss.outputs,y)\n",
    "    layer2.backward(act_loss.dinputs)\n",
    "    act1.backward(layer2.dinputs)\n",
    "    layer1.backward(act1.drelu)\n",
    "\n",
    "    #lets optimize the model\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f301dad-75d9-4f93-90a9-ebf676079993",
   "metadata": {},
   "source": [
    "# RMS-Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc545813-c012-4506-b5fb-e30e73964071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = RMS_Prop(decay_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db788039-695a-4e88-85c4-93398c499204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | accuracy is 0.39 |  loss is 1.099 | learning rate is 0.00100 \n",
      "epoch 1000 | accuracy is 0.41 |  loss is 1.066 | learning rate is 0.00091 \n",
      "epoch 2000 | accuracy is 0.39 |  loss is 1.065 | learning rate is 0.00083 \n",
      "epoch 3000 | accuracy is 0.41 |  loss is 1.063 | learning rate is 0.00077 \n",
      "epoch 4000 | accuracy is 0.40 |  loss is 1.063 | learning rate is 0.00071 \n",
      "epoch 5000 | accuracy is 0.40 |  loss is 1.062 | learning rate is 0.00067 \n",
      "epoch 6000 | accuracy is 0.40 |  loss is 1.062 | learning rate is 0.00063 \n",
      "epoch 7000 | accuracy is 0.40 |  loss is 1.062 | learning rate is 0.00059 \n",
      "epoch 8000 | accuracy is 0.40 |  loss is 1.061 | learning rate is 0.00056 \n",
      "epoch 9000 | accuracy is 0.39 |  loss is 1.061 | learning rate is 0.00053 \n",
      "epoch 10000 | accuracy is 0.40 |  loss is 1.060 | learning rate is 0.00050 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    #lets see the forward pass of the model\n",
    "    layer1.forward(x)\n",
    "    \n",
    "    act1.forward(layer1.outputs)\n",
    "    \n",
    "    layer2.forward(act1.output)\n",
    "    \n",
    "    loss=act_loss.forward(layer2.outputs,y)\n",
    "    \n",
    "    #lets display the loss of the model\n",
    "    \n",
    "    #lets calculate the accuracy of the model\n",
    "    prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "    \n",
    "    if len(y.shape)==2:\n",
    "        y=argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(prediction==y)\n",
    "\n",
    "    #lets print some\n",
    "    \n",
    "    #the accuracy of the model is\n",
    "    if i%1000==0:\n",
    "        print(f\"epoch {i} | accuracy is {accuracy:.2f} |  loss is {loss:.3f} | learning rate is {optimizer.current_learning_rate:.5f} \")\n",
    "    \n",
    "    #lets backward pass\n",
    "    act_loss.backward(act_loss.outputs,y)\n",
    "    layer2.backward(act_loss.dinputs)\n",
    "    act1.backward(layer2.dinputs)\n",
    "    layer1.backward(act1.drelu)\n",
    "\n",
    "    #lets optimize the model\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e6c2e-00c2-4c4d-a7b3-4b855e7909ab",
   "metadata": {},
   "source": [
    "## lets tweak the parameters of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d593382c-1f2e-4f41-8ce2-f0e14d49b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = RMS_Prop(lr=0.02,rho=0.99,decay_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e759fcd2-39df-4aef-b44f-1b5e2fd0458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | accuracy is 0.27 |  loss is 1.099 | learning rate is 0.02000 \n",
      "epoch 1000 | accuracy is 0.40 |  loss is 1.162 | learning rate is 0.01818 \n",
      "epoch 2000 | accuracy is 0.34 |  loss is 1.166 | learning rate is 0.01667 \n",
      "epoch 3000 | accuracy is 0.33 |  loss is 1.148 | learning rate is 0.01539 \n",
      "epoch 4000 | accuracy is 0.34 |  loss is 1.134 | learning rate is 0.01429 \n",
      "epoch 5000 | accuracy is 0.34 |  loss is 1.124 | learning rate is 0.01333 \n",
      "epoch 6000 | accuracy is 0.34 |  loss is 1.115 | learning rate is 0.01250 \n",
      "epoch 7000 | accuracy is 0.34 |  loss is 1.108 | learning rate is 0.01177 \n",
      "epoch 8000 | accuracy is 0.34 |  loss is 1.101 | learning rate is 0.01111 \n",
      "epoch 9000 | accuracy is 0.34 |  loss is 1.094 | learning rate is 0.01053 \n",
      "epoch 10000 | accuracy is 0.34 |  loss is 1.088 | learning rate is 0.01000 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    #lets see the forward pass of the model\n",
    "    layer1.forward(x)\n",
    "    \n",
    "    act1.forward(layer1.outputs)\n",
    "    \n",
    "    layer2.forward(act1.output)\n",
    "    \n",
    "    loss=act_loss.forward(layer2.outputs,y)\n",
    "    \n",
    "    #lets display the loss of the model\n",
    "    \n",
    "    #lets calculate the accuracy of the model\n",
    "    prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "    \n",
    "    if len(y.shape)==2:\n",
    "        y=argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(prediction==y)\n",
    "\n",
    "    #lets print some\n",
    "    \n",
    "    #the accuracy of the model is\n",
    "    if i%1000==0:\n",
    "        print(f\"epoch {i} | accuracy is {accuracy:.2f} |  loss is {loss:.3f} | learning rate is {optimizer.current_learning_rate:.5f} \")\n",
    "    \n",
    "    #lets backward pass\n",
    "    act_loss.backward(act_loss.outputs,y)\n",
    "    layer2.backward(act_loss.dinputs)\n",
    "    act1.backward(layer2.dinputs)\n",
    "    layer1.backward(act1.drelu)\n",
    "\n",
    "    #lets optimize the model\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27f72d-e7d3-45e3-b557-da184b102a53",
   "metadata": {},
   "source": [
    "# ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b628f9f-3b90-4deb-96df-01b35567a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=spiral_data(samples=1000,classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55ecceef-1832-4938-aa60-dcc9ae45ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the input layer\n",
    "layer1=Dense(2,64)\n",
    "\n",
    "#pass it through the activation layer\n",
    "act1=Relu()\n",
    "\n",
    "#the hidden layer\n",
    "layer2=Dense(64,3)\n",
    "\n",
    "#our loss and activation \n",
    "act_loss=softmax_categorical_loss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = Adam(lr=0.02,decay_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49752be-d40b-4b76-b45d-72316ff11e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | accuracy is 0.38 |  loss is 1.099 | learning rate is 0.02000 \n",
      "epoch 1000 | accuracy is 0.76 |  loss is 0.575 | learning rate is 0.01980 \n",
      "epoch 2000 | accuracy is 0.80 |  loss is 0.484 | learning rate is 0.01961 \n",
      "epoch 3000 | accuracy is 0.81 |  loss is 0.451 | learning rate is 0.01942 \n",
      "epoch 4000 | accuracy is 0.81 |  loss is 0.441 | learning rate is 0.01923 \n",
      "epoch 5000 | accuracy is 0.83 |  loss is 0.417 | learning rate is 0.01905 \n",
      "epoch 6000 | accuracy is 0.84 |  loss is 0.392 | learning rate is 0.01887 \n",
      "epoch 7000 | accuracy is 0.85 |  loss is 0.379 | learning rate is 0.01869 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    #lets see the forward pass of the model\n",
    "    layer1.forward(x)\n",
    "    \n",
    "    act1.forward(layer1.outputs)\n",
    "    \n",
    "    layer2.forward(act1.output)\n",
    "    \n",
    "    loss=act_loss.forward(layer2.outputs,y)\n",
    "    \n",
    "    #lets display the loss of the model\n",
    "    \n",
    "    #lets calculate the accuracy of the model\n",
    "    prediction=np.argmax(act_loss.outputs,axis=1)\n",
    "    \n",
    "    if len(y.shape)==2:\n",
    "        y=argmax(y,axis=1)\n",
    "    \n",
    "    accuracy=np.mean(prediction==y)\n",
    "\n",
    "    #lets print some\n",
    "    \n",
    "    #the accuracy of the model is\n",
    "    if i%1000==0:\n",
    "        print(f\"epoch {i} | accuracy is {accuracy:.2f} |  loss is {loss:.3f} | learning rate is {optimizer.current_learning_rate:.5f} \")\n",
    "    \n",
    "    #lets backward pass\n",
    "    act_loss.backward(act_loss.outputs,y)\n",
    "    layer2.backward(act_loss.dinputs)\n",
    "    act1.backward(layer2.dinputs)\n",
    "    layer1.backward(act1.drelu)\n",
    "\n",
    "    #lets optimize the model\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54c1f3-5233-4d2a-8624-f86d0ef3244c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
